"""
@file main.py
@description SNS í¬ë¡¤ë§ CLI ì¸í„°í˜ì´ìŠ¤

ì´ ëª¨ë“ˆì€ ì—¬ëŸ¬ SNS í”Œë«í¼ì˜ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•˜ê¸° ìœ„í•œ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. í”Œë«í¼ë³„ í¬ë¡¤ë§ ëª…ë ¹ì–´ (threads, linkedin, x, reddit)
2. í†µì¼ëœ í¬ë¡¤ë§ ì˜µì…˜ ì„¤ì • (ê²Œì‹œê¸€ ìˆ˜, ì €ì¥ ìœ„ì¹˜, ë””ë²„ê·¸ ëª¨ë“œ)
3. ì¼ê´€ëœ ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥

í•µì‹¬ êµ¬í˜„ ë¡œì§:
- Typerë¥¼ ì‚¬ìš©í•œ ì§ê´€ì ì¸ CLI ì¸í„°í˜ì´ìŠ¤
- ë¡œê¹… ë°ì½”ë ˆì´í„°ë¥¼ í†µí•œ í†µì¼ëœ ì‘ì—… ì¶”ì 
- ëª¨ë“  í”Œë«í¼ì—ì„œ ë™ì¼í•œ ì¶œë ¥ í˜•ì‹ ì œê³µ

@dependencies
- typer: CLI í”„ë ˆì„ì›Œí¬
- asyncio: ë¹„ë™ê¸° ì²˜ë¦¬
- datetime: íŒŒì¼ëª… ìƒì„±ìš©
- pathlib: ë””ë ‰í† ë¦¬ ê´€ë¦¬
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional

import typer

from src.crawlers.linkedin import LinkedInCrawler
from src.crawlers.reddit import RedditCrawler
from src.crawlers.threads import ThreadsCrawler
from src.crawlers.x import XCrawler
from src.exporters import SheetsExporter
from src.models import Post
from src.print import (
    log_crawl_operation,
    print_crawl_summary,
    print_no_posts_error,
    print_post_preview,
)

# === App Configuration ===
app = typer.Typer(
    name="crawl-sns",
    help="SNS í”Œë«í¼(Threads, LinkedIn, X, Reddit) í¬ë¡¤ë§ ë„êµ¬",
    add_completion=False,
    no_args_is_help=True,
    context_settings={"help_option_names": ["-h", "--help"]},
)

__version__ = "0.1.0"


# === Utility Functions ===
def save_posts_to_file(posts: List[Post], filepath: str) -> None:
    """ê²Œì‹œê¸€ ëª©ë¡ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    output_data = {
        "metadata": {
            "total_posts": len(posts),
            "crawled_at": datetime.now().isoformat(),
            "platform": posts[0].platform if posts else "unknown",
        },
        "posts": [post.model_dump() for post in posts],
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)


def generate_output_filename(platform: str, custom_output: Optional[str] = None) -> str:
    """ì¶œë ¥ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if custom_output:
        return custom_output

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"data/{platform}/{timestamp}.json"


def ensure_data_directory(platform: str = None) -> None:
    """data ë””ë ‰í† ë¦¬ì™€ í”Œë«í¼ë³„ í•˜ìœ„ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤."""
    Path("data").mkdir(exist_ok=True)
    if platform:
        Path(f"data/{platform}").mkdir(exist_ok=True)


# === Platform Crawling Commands ===
@app.command()
@log_crawl_operation("threads")
def threads(
    count: int = typer.Option(5, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    output: Optional[str] = typer.Option(
        None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸: ìë™ ìƒì„±)"
    ),
    debug: bool = typer.Option(
        False, "--debug", "-d", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ë¸Œë¼ìš°ì € í‘œì‹œ, ìƒì„¸ ë¡œê·¸, ìŠ¤í¬ë¦°ìƒ·)"
    ),
    sheets: bool = typer.Option(
        False, "--sheets", "-s", help="êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ (GOOGLE_WEBAPP_URL í™˜ê²½ë³€ìˆ˜ í•„ìš”)"
    ),
):
    """
    Threadsì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    python main.py threads --count 10
    python main.py threads -c 3 -o my_threads.json
    python main.py threads --debug  # ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
    python main.py threads --sheets  # êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
    python main.py threads -c 5 -s  # 5ê°œ ê²Œì‹œê¸€ì„ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
    """
    crawler = ThreadsCrawler(debug_mode=debug)
    posts = asyncio.run(crawler.crawl(count))

    if not posts:
        print_no_posts_error("threads", debug)
        raise typer.Exit(1)

    # JSON íŒŒì¼ ì €ì¥ (ê¸°ë³¸)
    ensure_data_directory("threads")
    output_file = generate_output_filename("threads", output)
    save_posts_to_file(posts, output_file)

    # êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ (ì˜µì…˜)
    sheets_success = False
    if sheets:
        try:
            exporter = SheetsExporter()
            sheets_success = exporter.export_posts(posts, "threads")
        except ValueError as e:
            typer.echo(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            sheets_success = False
        except Exception as e:
            typer.echo(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            sheets_success = False

    # ê²°ê³¼ ì¶œë ¥
    print_crawl_summary("threads", len(posts), output_file, debug)
    if sheets:
        if sheets_success:
            typer.echo("   ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥: âœ… ì„±ê³µ")
        else:
            typer.echo("   ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥: âŒ ì‹¤íŒ¨ (JSON íŒŒì¼ì€ ì €ì¥ë¨)")

    print_post_preview(posts[0], "threads")


@app.command()
@log_crawl_operation("linkedin")
def linkedin(
    count: int = typer.Option(5, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    output: Optional[str] = typer.Option(
        None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸: ìë™ ìƒì„±)"
    ),
    debug: bool = typer.Option(
        False, "--debug", "-d", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ë¸Œë¼ìš°ì € í‘œì‹œ, ìƒì„¸ ë¡œê·¸)"
    ),
    sheets: bool = typer.Option(
        False, "--sheets", "-s", help="êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ (GOOGLE_WEBAPP_URL í™˜ê²½ë³€ìˆ˜ í•„ìš”)"
    ),
):
    """
    LinkedInì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    python main.py linkedin --count 10
    python main.py linkedin -c 3 -o my_linkedin.json
    python main.py linkedin --debug  # ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
    python main.py linkedin --sheets  # êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
    python main.py linkedin -c 5 -s  # 5ê°œ ê²Œì‹œê¸€ì„ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
    """
    crawler = LinkedInCrawler(debug_mode=debug)
    posts = asyncio.run(crawler.crawl(count))

    if not posts:
        print_no_posts_error("linkedin", debug)
        raise typer.Exit(1)

    # JSON íŒŒì¼ ì €ì¥ (ê¸°ë³¸)
    ensure_data_directory("linkedin")
    output_file = generate_output_filename("linkedin", output)
    save_posts_to_file(posts, output_file)

    # êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ (ì˜µì…˜)
    sheets_success = False
    if sheets:
        try:
            exporter = SheetsExporter()
            sheets_success = exporter.export_posts(posts, "linkedin")
        except ValueError as e:
            typer.echo(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            sheets_success = False
        except Exception as e:
            typer.echo(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            sheets_success = False

    # ê²°ê³¼ ì¶œë ¥
    print_crawl_summary("linkedin", len(posts), output_file, debug)
    if sheets:
        if sheets_success:
            typer.echo("   ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥: âœ… ì„±ê³µ")
        else:
            typer.echo("   ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥: âŒ ì‹¤íŒ¨ (JSON íŒŒì¼ì€ ì €ì¥ë¨)")

    print_post_preview(posts[0], "linkedin")


@app.command()
@log_crawl_operation("x")
def x(
    count: int = typer.Option(10, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    output: Optional[str] = typer.Option(
        None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸: ìë™ ìƒì„±)"
    ),
    debug: bool = typer.Option(False, "--debug", "-d", help="ë””ë²„ê·¸ ëª¨ë“œ"),
    sheets: bool = typer.Option(
        False, "--sheets", "-s", help="êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ (GOOGLE_WEBAPP_URL í™˜ê²½ë³€ìˆ˜ í•„ìš”)"
    ),
):
    """
    X (Twitter)ì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    python main.py x --count 10
    python main.py x -c 5 -o my_x_posts.json
    python main.py x --debug
    python main.py x --sheets  # êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
    python main.py x -c 5 -s  # 5ê°œ ê²Œì‹œê¸€ì„ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
    """
    crawler = XCrawler(debug_mode=debug)
    posts = asyncio.run(crawler.crawl(count))

    if not posts:
        print_no_posts_error("x", debug)
        raise typer.Exit(1)

    # JSON íŒŒì¼ ì €ì¥ (ê¸°ë³¸)
    ensure_data_directory("x")
    output_file = generate_output_filename("x", output)
    save_posts_to_file(posts, output_file)

    # êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ (ì˜µì…˜)
    sheets_success = False
    if sheets:
        try:
            exporter = SheetsExporter()
            sheets_success = exporter.export_posts(posts, "x")
        except ValueError as e:
            typer.echo(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            sheets_success = False
        except Exception as e:
            typer.echo(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            sheets_success = False

    # ê²°ê³¼ ì¶œë ¥
    print_crawl_summary("x", len(posts), output_file, debug)
    if sheets:
        if sheets_success:
            typer.echo("   ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥: âœ… ì„±ê³µ")
        else:
            typer.echo("   ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥: âŒ ì‹¤íŒ¨ (JSON íŒŒì¼ì€ ì €ì¥ë¨)")

    print_post_preview(posts[0], "x")


@app.command()
@log_crawl_operation("reddit")
def reddit(
    count: int = typer.Option(10, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    output: Optional[str] = typer.Option(
        None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸: ìë™ ìƒì„±)"
    ),
    debug: bool = typer.Option(False, "--debug", "-d", help="ë””ë²„ê·¸ ëª¨ë“œ"),
    sheets: bool = typer.Option(
        False, "--sheets", "-s", help="êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥ (GOOGLE_WEBAPP_URL í™˜ê²½ë³€ìˆ˜ í•„ìš”)"
    ),
):
    """
    Redditì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    python main.py reddit --count 10
    python main.py reddit -c 5 -o my_reddit_posts.json
    python main.py reddit --debug
    python main.py reddit --sheets  # êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
    python main.py reddit -c 5 -s  # 5ê°œ ê²Œì‹œê¸€ì„ êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
    """
    crawler = RedditCrawler(debug_mode=debug)
    posts = asyncio.run(crawler.crawl(count))

    if not posts:
        print_no_posts_error("reddit", debug)
        raise typer.Exit(1)

    # JSON íŒŒì¼ ì €ì¥ (ê¸°ë³¸)
    ensure_data_directory("reddit")
    output_file = generate_output_filename("reddit", output)
    save_posts_to_file(posts, output_file)

    # êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ (ì˜µì…˜)
    sheets_success = False
    if sheets:
        try:
            exporter = SheetsExporter()
            sheets_success = exporter.export_posts(posts, "reddit")
        except ValueError as e:
            typer.echo(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            sheets_success = False
        except Exception as e:
            typer.echo(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            sheets_success = False

    # ê²°ê³¼ ì¶œë ¥
    print_crawl_summary("reddit", len(posts), output_file, debug)
    if sheets:
        if sheets_success:
            typer.echo("   ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥: âœ… ì„±ê³µ")
        else:
            typer.echo("   ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì €ì¥: âŒ ì‹¤íŒ¨ (JSON íŒŒì¼ì€ ì €ì¥ë¨)")

    print_post_preview(posts[0], "reddit")


# === Utility Commands ===
@app.command()
def version():
    """ë²„ì „ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    typer.echo(f"SNS Crawler v{__version__}")
    typer.echo("Playwright ê¸°ë°˜ í¬ë¡¤ë§ ë„êµ¬")


@app.command()
def status():
    """í˜„ì¬ í¬ë¡¤ëŸ¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    typer.echo("ğŸ“‹ SNS Crawler ìƒíƒœ:")
    typer.echo("   âœ… Threads í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ")
    typer.echo("   âœ… LinkedIn í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ")
    typer.echo("   ğŸ”§ X í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ")
    typer.echo("   ğŸ”§ Reddit í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ")


if __name__ == "__main__":
    app()
