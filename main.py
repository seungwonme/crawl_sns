"""
@file main.py
@description SNS í¬ë¡¤ë§ CLI ì¸í„°í˜ì´ìŠ¤

ì´ ëª¨ë“ˆì€ ì—¬ëŸ¬ SNS í”Œë«í¼ì˜ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•˜ê¸° ìœ„í•œ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. í”Œë«í¼ë³„ í¬ë¡¤ë§ ëª…ë ¹ì–´ (threads, linkedin, x, etc.)
2. í¬ë¡¤ë§ ì˜µì…˜ ì„¤ì • (ê²Œì‹œê¸€ ìˆ˜, ì €ì¥ ìœ„ì¹˜)
3. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥

í•µì‹¬ êµ¬í˜„ ë¡œì§:
- Typerë¥¼ ì‚¬ìš©í•œ ì§ê´€ì ì¸ CLI ì¸í„°í˜ì´ìŠ¤
- Playwrightë¥¼ ì‚¬ìš©í•œ ë¸Œë¼ìš°ì € ìë™í™” í¬ë¡¤ë§
- ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ìë™ ì €ì¥

@dependencies
- typer: CLI í”„ë ˆì„ì›Œí¬
- playwright: ë¸Œë¼ìš°ì € ìë™í™”
- asyncio: ë¹„ë™ê¸° ì²˜ë¦¬
- datetime: íŒŒì¼ëª… ìƒì„±ìš©
- json: ë°ì´í„° ì§ë ¬í™”
- pydantic: ë°ì´í„° ëª¨ë¸ë§
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional

import typer
from playwright.async_api import async_playwright

# Import crawler and models
from src.crawlers.threads import ThreadsCrawler
from src.models import Post

# === Version ===
__version__ = "0.1.0"


# === Data Models ===
# Post ëª¨ë¸ì€ src.modelsì—ì„œ importí•˜ì—¬ ì‚¬ìš©


# === Core Functions ===
def save_posts_to_file(posts: List[Post], filepath: str) -> None:
    """ê²Œì‹œê¸€ ëª©ë¡ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    output_data = {
        "metadata": {
            "total_posts": len(posts),
            "crawled_at": datetime.now().isoformat(),
            "platform": posts[0].platform if posts else "unknown",
        },
        "posts": [post.model_dump() for post in posts],
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)


# === Platform Crawlers ===


async def crawl_linkedin(count: int = 5) -> List[Post]:
    """LinkedInì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    posts = []

    try:
        typer.echo(f"ğŸ’¼ LinkedIn í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")

        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(headless=False)

            # User-Agentë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )

            page = await context.new_page()

            # LinkedIn í”¼ë“œ í˜ì´ì§€ë¡œ ì´ë™
            await page.goto("https://www.linkedin.com/feed/", wait_until="networkidle")
            typer.echo(f"âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ")

            # í˜ì´ì§€ ë¡œë“œ ì¶”ê°€ ëŒ€ê¸°
            await page.wait_for_timeout(3000)

            # LinkedInì€ ë¡œê·¸ì¸ì´ í•„ìš”í•œ ê²½ìš° ì²˜ë¦¬
            if await page.query_selector('input[name="session_key"]'):
                typer.echo(f"âš ï¸  LinkedInì€ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ê³µê°œ ê²Œì‹œê¸€ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                await page.goto("https://www.linkedin.com/posts/", wait_until="networkidle")
                await page.wait_for_timeout(2000)

            # ê²Œì‹œê¸€ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            post_elements = await page.query_selector_all(
                "div[data-id], article, .feed-shared-update-v2, .occludable-update"
            )

            # ëŒ€ì•ˆ ë°©ë²•: ë” ì¼ë°˜ì ì¸ ì„ íƒì ì‚¬ìš©
            if not post_elements:
                post_elements = await page.query_selector_all(
                    'div:has(a[href*="/posts/"]), div:has(a[href*="/activity-"])'
                )

            typer.echo(f"ğŸ” {len(post_elements)}ê°œì˜ ê²Œì‹œê¸€ ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤")

            # ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ
            for i, element in enumerate(post_elements[:count]):
                try:
                    # ì‘ì„±ì ì •ë³´ ì¶”ì¶œ
                    author_link = await element.query_selector(
                        'a[href*="/in/"], a[href*="/company/"]'
                    )
                    author = "Unknown"
                    if author_link:
                        text = await author_link.inner_text()
                        if text and text.strip():
                            author = text.strip().split("\n")[0]

                    # ê²Œì‹œê¸€ URL ì¶”ì¶œ
                    post_link = await element.query_selector(
                        'a[href*="/posts/"], a[href*="/activity-"]'
                    )
                    post_url = None
                    if post_link:
                        href = await post_link.get_attribute("href")
                        if href:
                            post_url = (
                                f"https://www.linkedin.com{href}"
                                if not href.startswith("http")
                                else href
                            )

                    # ê²Œì‹œ ì‹œê°„ ì¶”ì¶œ
                    time_element = await element.query_selector(
                        'time, span:has-text("ago"), span:has-text("ì¼"), span:has-text("ì‹œê°„")'
                    )
                    timestamp = "ì•Œ ìˆ˜ ì—†ìŒ"
                    if time_element:
                        time_text = await time_element.inner_text()
                        if time_text:
                            timestamp = time_text.strip()

                    # ì½˜í…ì¸  ì¶”ì¶œ
                    content_text = ""

                    # LinkedIn ê²Œì‹œê¸€ ì½˜í…ì¸  ì„ íƒìë“¤
                    content_selectors = [
                        ".feed-shared-text",
                        ".feed-shared-inline-show-more-text",
                        'div[data-test-id="main-feed-activity-card"] span[dir="ltr"]',
                        ".break-words span",
                    ]

                    for selector in content_selectors:
                        content_element = await element.query_selector(selector)
                        if content_element:
                            text = await content_element.inner_text()
                            if text and len(text.strip()) > 20:
                                content_text = text.strip()
                                break

                    # ëŒ€ì•ˆ: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                    if not content_text:
                        full_text = await element.inner_text()
                        if full_text:
                            lines = full_text.split("\n")
                            for line in lines:
                                line = line.strip()
                                if (
                                    len(line) > 30
                                    and not any(
                                        exclude in line.lower()
                                        for exclude in [
                                            "like",
                                            "comment",
                                            "share",
                                            "repost",
                                            "follow",
                                            "connection",
                                        ]
                                    )
                                    and not line.isdigit()
                                ):
                                    content_text = line
                                    break

                    # ìƒí˜¸ì‘ìš© ì •ë³´ ì¶”ì¶œ
                    likes = 0
                    comments = 0

                    # ì¢‹ì•„ìš” ìˆ˜ (LinkedInì€ ë‹¤ì–‘í•œ ë¦¬ì•¡ì…˜ í¬í•¨)
                    reaction_elements = await element.query_selector_all(
                        'button:has-text("reaction"), span:has-text("reaction"), .social-action'
                    )
                    for elem in reaction_elements:
                        try:
                            text = await elem.inner_text()
                            numbers = "".join(filter(str.isdigit, text))
                            if numbers:
                                likes = int(numbers)
                                break
                        except:
                            pass

                    # ëŒ“ê¸€ ìˆ˜
                    comment_elements = await element.query_selector_all(
                        'button:has-text("comment"), span:has-text("comment")'
                    )
                    for elem in comment_elements:
                        try:
                            text = await elem.inner_text()
                            numbers = "".join(filter(str.isdigit, text))
                            if numbers:
                                comments = int(numbers)
                                break
                        except:
                            pass

                    # ìœ íš¨í•œ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸
                    if content_text and len(content_text.strip()) > 15 and author != "Unknown":
                        post = Post(
                            platform="linkedin",
                            author=author,
                            content=content_text[:500],
                            timestamp=timestamp,
                            url=post_url,
                            likes=likes if likes > 0 else None,
                            comments=comments if comments > 0 else None,
                            shares=None,
                        )
                        posts.append(post)
                        typer.echo(f"   âœ… ê²Œì‹œê¸€ {len(posts)}: {author} - {content_text[:50]}...")
                    else:
                        typer.echo(
                            f"   âš ï¸  ê²Œì‹œê¸€ {i+1}: ë°ì´í„° ë¶€ì¡± - author={author}, content_len={len(content_text) if content_text else 0}"
                        )

                except Exception as e:
                    typer.echo(f"   âŒ ê²Œì‹œê¸€ {i+1} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            await browser.close()
            typer.echo(f"ğŸ“Š ì´ {len(posts)}ê°œì˜ ê²Œì‹œê¸€ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")

            if not posts:
                typer.echo(f"âŒ ê²Œì‹œê¸€ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                typer.echo(f"ğŸ’¡ íŒíŠ¸: LinkedInì€ ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        typer.echo(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return posts


async def crawl_x(count: int = 5) -> List[Post]:
    """X (Twitter) í¬ë¡¤ë§ (êµ¬í˜„ ì˜ˆì •)"""
    typer.echo(f"ğŸ¦ X í¬ë¡¤ë§ êµ¬í˜„ ì˜ˆì •")
    return []


async def crawl_geeknews(count: int = 5) -> List[Post]:
    """GeekNewsì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    posts = []

    try:
        typer.echo(f"ğŸ¤“ GeekNews í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")

        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = await p.chromium.launch(headless=False)

            # User-Agentë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )

            page = await context.new_page()

            # GeekNews ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            await page.goto("https://news.hada.io/", wait_until="networkidle")
            typer.echo(f"âœ… í˜ì´ì§€ ë¡œë“œ ì„±ê³µ")

            # í˜ì´ì§€ ë¡œë“œ ì¶”ê°€ ëŒ€ê¸°
            await page.wait_for_timeout(2000)

            # GeekNews ê²Œì‹œê¸€ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            post_elements = await page.query_selector_all(
                ".topic_row, .article-item, .news-item, .post-item"
            )

            # ëŒ€ì•ˆ: ë§í¬ ê¸°ë°˜ìœ¼ë¡œ ì°¾ê¸°
            if not post_elements:
                post_elements = await page.query_selector_all('div:has(a[href*="/topic/"])')

            # ë” ì¼ë°˜ì ì¸ ë°©ë²•: ì œëª©ì´ ìˆëŠ” ìš”ì†Œë“¤
            if not post_elements:
                title_links = await page.query_selector_all('a[href*="/topic/"]')
                containers = []
                for link in title_links[: count * 2]:
                    try:
                        # ìƒìœ„ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                        container = await link.evaluate_handle(
                            """(element) => {
                            let current = element;
                            for (let i = 0; i < 4; i++) {
                                if (current.parentElement) {
                                    current = current.parentElement;
                                    if (current.textContent && current.textContent.length > 50) {
                                        return current;
                                    }
                                }
                            }
                            return null;
                        }"""
                        )

                        if container:
                            element = container.as_element()
                            if element and element not in containers:
                                containers.append(element)

                    except Exception:
                        continue

                post_elements = containers[:count]

            typer.echo(f"ğŸ” {len(post_elements)}ê°œì˜ ê²Œì‹œê¸€ ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤")

            # ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ
            for i, element in enumerate(post_elements[:count]):
                try:
                    # ì œëª©/ì½˜í…ì¸  ì¶”ì¶œ (GeekNewsëŠ” ì œëª©ì´ ì£¼ìš” ì½˜í…ì¸ )
                    title_link = await element.query_selector('a[href*="/topic/"]')
                    content_text = ""
                    post_url = None

                    if title_link:
                        content_text = await title_link.inner_text()
                        href = await title_link.get_attribute("href")
                        if href:
                            post_url = (
                                f"https://news.hada.io{href}"
                                if not href.startswith("http")
                                else href
                            )

                    # ì‘ì„±ì ì •ë³´ ì¶”ì¶œ (GeekNewsëŠ” ë³´í†µ ì‘ì„±ìê°€ ëª…ì‹œë˜ì§€ ì•ŠìŒ)
                    author = "GeekNews"
                    author_element = await element.query_selector(".author, .user, .by")
                    if author_element:
                        author_text = await author_element.inner_text()
                        if author_text and author_text.strip():
                            author = author_text.strip()

                    # ê²Œì‹œ ì‹œê°„ ì¶”ì¶œ
                    time_element = await element.query_selector(
                        'time, .time, .date, span:has-text("ì‹œê°„"), span:has-text("ì¼"), span:has-text("ago")'
                    )
                    timestamp = "ì•Œ ìˆ˜ ì—†ìŒ"
                    if time_element:
                        time_text = await time_element.inner_text()
                        if time_text:
                            timestamp = time_text.strip()

                    # ëŒ“ê¸€ ìˆ˜ë‚˜ ì ìˆ˜ ì¶”ì¶œ
                    comments = 0
                    likes = 0

                    # ëŒ“ê¸€ ìˆ˜ ì°¾ê¸°
                    comment_elements = await element.query_selector_all(
                        'span:has-text("ëŒ“ê¸€"), .comment, a[href*="#comment"]'
                    )
                    for elem in comment_elements:
                        try:
                            text = await elem.inner_text()
                            numbers = "".join(filter(str.isdigit, text))
                            if numbers:
                                comments = int(numbers)
                                break
                        except:
                            pass

                    # ì ìˆ˜ë‚˜ ì¶”ì²œ ìˆ˜ ì°¾ê¸°
                    score_elements = await element.query_selector_all(".score, .points, .vote")
                    for elem in score_elements:
                        try:
                            text = await elem.inner_text()
                            numbers = "".join(filter(str.isdigit, text))
                            if numbers:
                                likes = int(numbers)
                                break
                        except:
                            pass

                    # ì¶”ê°€ ì„¤ëª…ì´ë‚˜ ìš”ì•½ ì¶”ì¶œ
                    description_element = await element.query_selector(
                        ".description, .summary, .excerpt"
                    )
                    if description_element:
                        desc_text = await description_element.inner_text()
                        if desc_text and len(desc_text.strip()) > 10:
                            content_text += f"\n{desc_text.strip()}"

                    # ìœ íš¨í•œ ê²Œì‹œê¸€ì¸ì§€ í™•ì¸
                    if content_text and len(content_text.strip()) > 10:
                        post = Post(
                            platform="geeknews",
                            author=author,
                            content=content_text.strip()[:500],
                            timestamp=timestamp,
                            url=post_url,
                            likes=likes if likes > 0 else None,
                            comments=comments if comments > 0 else None,
                            shares=None,
                        )
                        posts.append(post)
                        typer.echo(f"   âœ… ê²Œì‹œê¸€ {len(posts)}: {author} - {content_text[:50]}...")
                    else:
                        typer.echo(
                            f"   âš ï¸  ê²Œì‹œê¸€ {i+1}: ë°ì´í„° ë¶€ì¡± - content_len={len(content_text) if content_text else 0}"
                        )

                except Exception as e:
                    typer.echo(f"   âŒ ê²Œì‹œê¸€ {i+1} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue

            await browser.close()
            typer.echo(f"ğŸ“Š ì´ {len(posts)}ê°œì˜ ê²Œì‹œê¸€ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")

            if not posts:
                typer.echo(f"âŒ ê²Œì‹œê¸€ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                typer.echo(f"ğŸ’¡ GeekNews ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        typer.echo(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return posts


async def crawl_reddit(count: int = 5) -> List[Post]:
    """Reddit í¬ë¡¤ë§ (êµ¬í˜„ ì˜ˆì •)"""
    typer.echo(f"ğŸ”¸ Reddit í¬ë¡¤ë§ êµ¬í˜„ ì˜ˆì •")
    return []


# === CLI Interface ===
app = typer.Typer(
    name="crawl-sns",
    help="SNS í”Œë«í¼(Threads, LinkedIn, X, GeekNews, Reddit) í¬ë¡¤ë§ ë„êµ¬",
    add_completion=False,
)


@app.command()
def threads(
    count: int = typer.Option(5, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    output: Optional[str] = typer.Option(
        None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸: ìë™ ìƒì„±)"
    ),
    debug: bool = typer.Option(
        False, "--debug", "-d", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ë¸Œë¼ìš°ì € í‘œì‹œ, ìƒì„¸ ë¡œê·¸, ìŠ¤í¬ë¦°ìƒ·)"
    ),
):
    """
    Threadsì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    python main.py threads --count 10
    python main.py threads -c 3 -o my_threads.json
    python main.py threads --debug  # ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
    """
    if debug:
        typer.echo(f"ğŸ› ë””ë²„ê·¸ ëª¨ë“œë¡œ Threads í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")
        typer.echo("   - ë¸Œë¼ìš°ì €ê°€ í‘œì‹œë©ë‹ˆë‹¤")
        typer.echo("   - ìƒì„¸í•œ ë¡œê·¸ì™€ ìŠ¤í¬ë¦°ìƒ·ì´ ì €ì¥ë©ë‹ˆë‹¤")
        typer.echo("   - ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    else:
        typer.echo(f"ğŸ§µ Threads í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")

    # ThreadsCrawler í´ë˜ìŠ¤ì— debug_mode ì „ë‹¬
    crawler = ThreadsCrawler(debug_mode=debug)
    posts = asyncio.run(crawler.crawl(count))

    if not posts:
        typer.echo("âŒ í¬ë¡¤ë§ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        if debug:
            typer.echo("ğŸ’¡ ë””ë²„ê·¸ ì •ë³´:")
            typer.echo("   - data/debug_screenshots/ í´ë”ì˜ ìŠ¤í¬ë¦°ìƒ·ì„ í™•ì¸í•´ë³´ì„¸ìš”")
            typer.echo(
                "   - í™˜ê²½ ë³€ìˆ˜ THREADS_USERNAME, THREADS_PASSWORDê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”"
            )
            typer.echo("   - ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ ì˜¤ë¥˜ ì›ì¸ì„ ì°¾ì•„ë³´ì„¸ìš”")
        raise typer.Exit(1)

    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    if not output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = f"data/threads_{timestamp}.json"

    # data ë””ë ‰í† ë¦¬ ìƒì„±
    Path("data").mkdir(exist_ok=True)

    # ê²°ê³¼ ì €ì¥
    save_posts_to_file(posts, output)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    typer.echo(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½:")
    typer.echo(f"   - í”Œë«í¼: Threads")
    typer.echo(f"   - ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(posts)}ê°œ")
    typer.echo(f"   - ì €ì¥ ìœ„ì¹˜: {output}")
    if debug:
        typer.echo(f"   - ë””ë²„ê·¸ ìŠ¤í¬ë¦°ìƒ·: data/debug_screenshots/")

    # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°
    if posts:
        first_post = posts[0]
        typer.echo(f"\nğŸ“„ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°:")
        typer.echo(f"   ì‘ì„±ì: {first_post.author}")
        typer.echo(f"   ë‚´ìš©: {first_post.content[:100]}...")
        typer.echo(f"   ì‹œê°„: {first_post.timestamp}")


@app.command()
def linkedin(
    count: int = typer.Option(5, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    output: Optional[str] = typer.Option(
        None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸: ìë™ ìƒì„±)"
    ),
):
    """
    LinkedInì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    python main.py linkedin --count 10
    python main.py linkedin -c 3 -o my_linkedin.json
    """
    typer.echo(f"ğŸ’¼ LinkedIn í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")

    # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
    posts = asyncio.run(crawl_linkedin(count))

    if not posts:
        typer.echo("âŒ í¬ë¡¤ë§ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        raise typer.Exit(1)

    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    if not output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = f"data/linkedin_{timestamp}.json"

    # data ë””ë ‰í† ë¦¬ ìƒì„±
    Path("data").mkdir(exist_ok=True)

    # ê²°ê³¼ ì €ì¥
    save_posts_to_file(posts, output)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    typer.echo(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½:")
    typer.echo(f"   - í”Œë«í¼: LinkedIn")
    typer.echo(f"   - ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(posts)}ê°œ")
    typer.echo(f"   - ì €ì¥ ìœ„ì¹˜: {output}")

    # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°
    if posts:
        first_post = posts[0]
        typer.echo(f"\nğŸ“„ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°:")
        typer.echo(f"   ì‘ì„±ì: {first_post.author}")
        typer.echo(f"   ë‚´ìš©: {first_post.content[:100]}...")
        typer.echo(f"   ì‹œê°„: {first_post.timestamp}")


@app.command()
def version():
    """ë²„ì „ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    typer.echo("SNS Crawler v0.1.0")
    typer.echo("Playwright ê¸°ë°˜ í¬ë¡¤ë§ ë„êµ¬")


@app.command()
def status():
    """í˜„ì¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    typer.echo("ğŸ“‹ SNS Crawler ìƒíƒœ:")
    typer.echo("   âœ… Threads í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ (Playwright ê¸°ë°˜)")
    typer.echo("   ğŸ”§ LinkedIn í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ (í…ŒìŠ¤íŠ¸ í•„ìš”)")
    typer.echo("   ğŸ”§ GeekNews í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ (í…ŒìŠ¤íŠ¸ í•„ìš”)")
    typer.echo("   â³ X í¬ë¡¤ëŸ¬ - ê°œë°œ ì˜ˆì •")
    typer.echo("   â³ Reddit í¬ë¡¤ëŸ¬ - ê°œë°œ ì˜ˆì •")


if __name__ == "__main__":
    app()
