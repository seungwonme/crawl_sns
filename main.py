"""
@file main.py
@description SNS í¬ë¡¤ë§ CLI ì¸í„°í˜ì´ìŠ¤

ì´ ëª¨ë“ˆì€ ì—¬ëŸ¬ SNS í”Œë«í¼ì˜ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•˜ê¸° ìœ„í•œ ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. í”Œë«í¼ë³„ í¬ë¡¤ë§ ëª…ë ¹ì–´ (threads, linkedin, x, etc.)
2. í¬ë¡¤ë§ ì˜µì…˜ ì„¤ì • (ê²Œì‹œê¸€ ìˆ˜, ì €ì¥ ìœ„ì¹˜)
3. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥

í•µì‹¬ êµ¬í˜„ ë¡œì§:
- Typerë¥¼ ì‚¬ìš©í•œ ì§ê´€ì ì¸ CLI ì¸í„°í˜ì´ìŠ¤
- Playwrightë¥¼ ì‚¬ìš©í•œ ë¸Œë¼ìš°ì € ìë™í™” í¬ë¡¤ë§
- ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ìë™ ì €ì¥

@dependencies
- typer: CLI í”„ë ˆì„ì›Œí¬
- playwright: ë¸Œë¼ìš°ì € ìë™í™”
- asyncio: ë¹„ë™ê¸° ì²˜ë¦¬
- datetime: íŒŒì¼ëª… ìƒì„±ìš©
- json: ë°ì´í„° ì§ë ¬í™”
- pydantic: ë°ì´í„° ëª¨ë¸ë§
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional

import typer
from playwright.async_api import async_playwright

from src.crawlers.linkedin import LinkedInCrawler
from src.crawlers.threads import ThreadsCrawler
from src.crawlers.x import XCrawler
from src.models import Post

# === Version ===
__version__ = "0.1.0"


# === Data Models ===
# Post ëª¨ë¸ì€ src.modelsì—ì„œ importí•˜ì—¬ ì‚¬ìš©


# === Core Functions ===
def save_posts_to_file(posts: List[Post], filepath: str) -> None:
    """ê²Œì‹œê¸€ ëª©ë¡ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    output_data = {
        "metadata": {
            "total_posts": len(posts),
            "crawled_at": datetime.now().isoformat(),
            "platform": posts[0].platform if posts else "unknown",
        },
        "posts": [post.model_dump() for post in posts],
    }

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)


# === Platform Crawlers ===


async def crawl_x(count: int = 5) -> List[Post]:
    """X (Twitter) í¬ë¡¤ë§ (êµ¬í˜„ ì˜ˆì •)"""
    typer.echo(f"ğŸ¦ X í¬ë¡¤ë§ êµ¬í˜„ ì˜ˆì •")
    return []


async def crawl_geeknews(count: int = 5) -> List[Post]:
    """GeekNews í¬ë¡¤ë§ (êµ¬í˜„ ì˜ˆì •)"""
    typer.echo(f"ğŸ”¸ GeekNews í¬ë¡¤ë§ êµ¬í˜„ ì˜ˆì •")
    return []


async def crawl_reddit(count: int = 5) -> List[Post]:
    """Reddit í¬ë¡¤ë§ (êµ¬í˜„ ì˜ˆì •)"""
    typer.echo(f"ğŸ”¸ Reddit í¬ë¡¤ë§ êµ¬í˜„ ì˜ˆì •")
    return []


# === CLI Interface ===
app = typer.Typer(
    name="crawl-sns",
    help="SNS í”Œë«í¼(Threads, LinkedIn, X, GeekNews, Reddit) í¬ë¡¤ë§ ë„êµ¬",
    add_completion=False,
)


@app.command()
def threads(
    count: int = typer.Option(5, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    output: Optional[str] = typer.Option(
        None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸: ìë™ ìƒì„±)"
    ),
    debug: bool = typer.Option(
        False, "--debug", "-d", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ë¸Œë¼ìš°ì € í‘œì‹œ, ìƒì„¸ ë¡œê·¸, ìŠ¤í¬ë¦°ìƒ·)"
    ),
):
    """
    Threadsì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    python main.py threads --count 10
    python main.py threads -c 3 -o my_threads.json
    python main.py threads --debug  # ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
    """
    if debug:
        typer.echo(f"ğŸ› ë””ë²„ê·¸ ëª¨ë“œë¡œ Threads í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")
        typer.echo("   - ë¸Œë¼ìš°ì €ê°€ í‘œì‹œë©ë‹ˆë‹¤")
        typer.echo("   - ìƒì„¸í•œ ë¡œê·¸ì™€ ìŠ¤í¬ë¦°ìƒ·ì´ ì €ì¥ë©ë‹ˆë‹¤")
        typer.echo("   - ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
    else:
        typer.echo(f"ğŸ§µ Threads í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")

    # ThreadsCrawler í´ë˜ìŠ¤ì— debug_mode ì „ë‹¬
    crawler = ThreadsCrawler(debug_mode=debug)
    posts = asyncio.run(crawler.crawl(count))

    if not posts:
        typer.echo("âŒ í¬ë¡¤ë§ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        if debug:
            typer.echo("ğŸ’¡ ë””ë²„ê·¸ ì •ë³´:")
            typer.echo("   - data/debug_screenshots/ í´ë”ì˜ ìŠ¤í¬ë¦°ìƒ·ì„ í™•ì¸í•´ë³´ì„¸ìš”")
            typer.echo(
                "   - í™˜ê²½ ë³€ìˆ˜ THREADS_USERNAME, THREADS_PASSWORDê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”"
            )
            typer.echo("   - ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ ì˜¤ë¥˜ ì›ì¸ì„ ì°¾ì•„ë³´ì„¸ìš”")
        raise typer.Exit(1)

    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    if not output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = f"data/threads_{timestamp}.json"

    # data ë””ë ‰í† ë¦¬ ìƒì„±
    Path("data").mkdir(exist_ok=True)

    # ê²°ê³¼ ì €ì¥
    save_posts_to_file(posts, output)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    typer.echo(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½:")
    typer.echo(f"   - í”Œë«í¼: Threads")
    typer.echo(f"   - ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(posts)}ê°œ")
    typer.echo(f"   - ì €ì¥ ìœ„ì¹˜: {output}")
    if debug:
        typer.echo(f"   - ë””ë²„ê·¸ ìŠ¤í¬ë¦°ìƒ·: data/debug_screenshots/")


@app.command()
def linkedin(
    count: int = typer.Option(5, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    output: Optional[str] = typer.Option(
        None, "--output", "-o", help="ì¶œë ¥ íŒŒì¼ëª… (ê¸°ë³¸: ìë™ ìƒì„±)"
    ),
    debug: bool = typer.Option(
        False, "--debug", "-d", help="ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ë¸Œë¼ìš°ì € í‘œì‹œ, ìƒì„¸ ë¡œê·¸)"
    ),
):
    """
    LinkedInì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ:
    python main.py linkedin --count 10
    python main.py linkedin -c 3 -o my_linkedin.json
    python main.py linkedin --debug  # ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
    """
    if debug:
        typer.echo(f"ğŸ› ë””ë²„ê·¸ ëª¨ë“œë¡œ LinkedIn í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")
        typer.echo("   - ë¸Œë¼ìš°ì €ê°€ í‘œì‹œë©ë‹ˆë‹¤")
        typer.echo("   - ìƒì„¸í•œ ë¡œê·¸ê°€ ì¶œë ¥ë©ë‹ˆë‹¤")
    else:
        typer.echo(f"ğŸ’¼ LinkedIn í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê²Œì‹œê¸€ {count}ê°œ)")

    # LinkedInCrawler í´ë˜ìŠ¤ì— debug_mode ì „ë‹¬
    crawler = LinkedInCrawler(debug_mode=debug)
    posts = asyncio.run(crawler.crawl(count))

    if not posts:
        typer.echo("âŒ í¬ë¡¤ë§ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        if debug:
            typer.echo("ğŸ’¡ ë””ë²„ê·¸ ì •ë³´:")
            typer.echo("   - ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”")
            typer.echo(
                "   - í™˜ê²½ ë³€ìˆ˜ LINKEDIN_USERNAME, LINKEDIN_PASSWORDê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”"
            )
            typer.echo("   - ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ ì˜¤ë¥˜ ì›ì¸ì„ ì°¾ì•„ë³´ì„¸ìš”")
        raise typer.Exit(1)

    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    if not output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output = f"data/linkedin_{timestamp}.json"

    # data ë””ë ‰í† ë¦¬ ìƒì„±
    Path("data").mkdir(exist_ok=True)

    # ê²°ê³¼ ì €ì¥
    save_posts_to_file(posts, output)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    typer.echo(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½:")
    typer.echo(f"   - í”Œë«í¼: LinkedIn")
    typer.echo(f"   - ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(posts)}ê°œ")
    typer.echo(f"   - ì €ì¥ ìœ„ì¹˜: {output}")
    if debug:
        typer.echo(f"   - ë””ë²„ê·¸ ì„¸ì…˜: data/linkedin_session.json")

    # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°
    if posts:
        first_post = posts[0]
        typer.echo(f"\nğŸ“„ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°:")
        typer.echo(f"   ì‘ì„±ì: {first_post.author}")
        typer.echo(f"   ë‚´ìš©: {first_post.content[:100]}...")
        typer.echo(f"   ì‹œê°„: {first_post.timestamp}")
        if first_post.likes:
            typer.echo(f"   ì¢‹ì•„ìš”: {first_post.likes}")
        if first_post.comments:
            typer.echo(f"   ëŒ“ê¸€: {first_post.comments}")
        if first_post.shares:
            typer.echo(f"   ê³µìœ : {first_post.shares}")
        if first_post.views:
            typer.echo(f"   ì¡°íšŒìˆ˜: {first_post.views}")


@app.command()
def x(
    count: int = typer.Option(10, "--count", "-c", help="ìˆ˜ì§‘í•  ê²Œì‹œê¸€ ìˆ˜"),
    debug: bool = typer.Option(False, "--debug", "-d", help="ë””ë²„ê·¸ ëª¨ë“œ"),
):
    """X (Twitter)ì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤"""
    typer.echo(f"ğŸ¦ X í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {count}ê°œ ê²Œì‹œê¸€)")

    crawler = XCrawler(debug_mode=debug)
    posts = asyncio.run(crawler.crawl(count))

    if posts:
        typer.echo(f"âœ… {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ!")

        # ë°ì´í„° ì €ì¥
        data = [post.model_dump() for post in posts]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/x_{timestamp}.json"

        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        Path("data").mkdir(exist_ok=True)

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)

        typer.echo(f"ğŸ“ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")

        # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°
        if posts:
            first_post = posts[0]
            typer.echo(f"\nğŸ“ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë¯¸ë¦¬ë³´ê¸°:")
            typer.echo(f"   ì‘ì„±ì: {first_post.author}")
            typer.echo(f"   ë‚´ìš©: {first_post.content[:100]}...")
            typer.echo(f"   ì‹œê°„: {first_post.timestamp}")
            if first_post.likes:
                typer.echo(f"   ì¢‹ì•„ìš”: {first_post.likes}")
            if first_post.comments:
                typer.echo(f"   ëŒ“ê¸€: {first_post.comments}")
            if first_post.shares:
                typer.echo(f"   ê³µìœ : {first_post.shares}")
            if first_post.views:
                typer.echo(f"   ì¡°íšŒìˆ˜: {first_post.views}")
    else:
        typer.echo("âŒ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")


@app.command()
def version():
    """ë²„ì „ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    typer.echo("SNS Crawler v0.1.0")
    typer.echo("Playwright ê¸°ë°˜ í¬ë¡¤ë§ ë„êµ¬")


@app.command()
def status():
    """í˜„ì¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    typer.echo("ğŸ“‹ SNS Crawler ìƒíƒœ:")
    typer.echo("   âœ… Threads í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ")
    typer.echo("   âœ… LinkedIn í¬ë¡¤ëŸ¬ - êµ¬í˜„ ì™„ë£Œ")
    typer.echo("   â³ X í¬ë¡¤ëŸ¬ - ê°œë°œ ì˜ˆì •")
    typer.echo("   â³ Reddit í¬ë¡¤ëŸ¬ - ê°œë°œ ì˜ˆì •")
    typer.echo("   â³ GeekNews í¬ë¡¤ëŸ¬ - ê°œë°œ ì˜ˆì •")


if __name__ == "__main__":
    app()
