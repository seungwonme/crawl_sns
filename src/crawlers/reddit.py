"""
@file reddit.py
@description Reddit í”Œë«í¼ ì „ìš© í¬ë¡¤ëŸ¬

ì´ ëª¨ë“ˆì€ Reddit í”Œë«í¼ì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. Reddit í”¼ë“œì—ì„œ ê²Œì‹œê¸€ ìˆ˜ì§‘
2. Reddit ê³„ì •ì„ í†µí•œ ë¡œê·¸ì¸ ì§€ì›
3. ì‘ì„±ì, ì½˜í…ì¸ , ì—…ë³´íŠ¸/ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ
4. ì„¸ì…˜ ê´€ë¦¬ (ì¬ë¡œê·¸ì¸ ë°©ì§€)
5. ì ì§„ì  ì¶”ì¶œ ì‹œìŠ¤í…œ (ìŠ¤í¬ë¡¤ë§, ì„œë¸Œë ˆë”§ íƒìƒ‰)

í•µì‹¬ êµ¬í˜„ ë¡œì§:
- Reddit ë¡œê·¸ì¸ì„ í†µí•œ í”¼ë“œ ì ‘ê·¼
- ì ì§„ì  ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ë” ë§ì€ ê²Œì‹œê¸€ ë¡œë“œ
- div[data-testid="post-container"] ê¸°ë°˜ ê²Œì‹œê¸€ ì¶”ì¶œ
- K/M ë‹¨ìœ„ ì—…ë³´íŠ¸ ìˆ˜ì¹˜ íŒŒì‹±

@dependencies
- playwright.async_api: ë¸Œë¼ìš°ì € ìë™í™”
- typer: CLI ì¶œë ¥
- .base: ë² ì´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤

@see {@link https://www.reddit.com} - Reddit í”Œë«í¼
"""

import os
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import typer
from dotenv import load_dotenv
from playwright.async_api import Page
from playwright.async_api import TimeoutError as PlaywrightTimeoutError

from src.crawlers.base import BaseCrawler
from src.models import Post

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class RedditCrawler(BaseCrawler):
    """Reddit ì „ìš© í¬ë¡¤ëŸ¬"""

    def __init__(self, debug_mode: bool = False):
        super().__init__(
            platform_name="reddit", base_url="https://www.reddit.com", debug_mode=debug_mode
        )
        self.username = os.getenv("REDDIT_USERNAME")
        self.password = os.getenv("REDDIT_PASSWORD")
        self.session_path = Path("data/sessions/reddit_session.json")
        self.max_scroll_attempts = 10

        if not self.username or not self.password:
            raise ValueError("REDDIT_USERNAMEê³¼ REDDIT_PASSWORD í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    async def _crawl_implementation(self, page: Page, count: int) -> List[Post]:
        """Reddit ê²Œì‹œê¸€ í¬ë¡¤ë§ êµ¬í˜„"""
        try:
            # ì„¸ì…˜ ë¡œë“œ ì‹œë„
            if not await self._load_session(page):
                # ì„¸ì…˜ì´ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ë¡œê·¸ì¸
                await self._login(page)
                await self._save_session(page)

            # ë©”ì¸ í”¼ë“œë¡œ ì´ë™
            await page.goto("https://www.reddit.com/", wait_until="domcontentloaded")
            await page.wait_for_timeout(3000)

            # ê²Œì‹œê¸€ ìˆ˜ì§‘
            posts = await self._progressive_post_collection(page, count)

            if self.debug_mode:
                await self._save_debug_html(page, "reddit_posts.html")

            return posts

        except Exception as e:
            typer.echo(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_error.html")
            return []

    async def crawl(self, count: int = 10) -> List[Post]:
        """Reddit ê²Œì‹œê¸€ í¬ë¡¤ë§ - ë² ì´ìŠ¤ í´ë˜ìŠ¤ ì˜¤ë²„ë¼ì´ë“œ"""
        typer.echo(f"ğŸ”´ Reddit í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {count}ê°œ)")
        return await super().crawl(count)

    async def _fill_login_field(
        self, page: Page, selectors: List[str], value: str, field_name: str
    ) -> bool:
        """ë¡œê·¸ì¸ í•„ë“œ ì…ë ¥ í—¬í¼ ë©”ì„œë“œ"""
        for selector in selectors:
            try:
                input_field = await page.wait_for_selector(selector, timeout=3000)
                if input_field and value:
                    await input_field.fill(value)
                    typer.echo(f"   âœ… {field_name} ì…ë ¥ ì™„ë£Œ")
                    return True
            except PlaywrightTimeoutError:
                continue
        typer.echo(f"   âŒ {field_name} ì…ë ¥ í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return False

    async def _click_login_button(self, page: Page) -> bool:
        """ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ í—¬í¼ ë©”ì„œë“œ"""
        login_button_selectors = [
            'button[type="submit"]',
            'button:has-text("Log In")',
            'button:has-text("Sign In")',
            ".login-button",
            '[data-testid="login-button"]',
        ]

        for selector in login_button_selectors:
            try:
                login_button = await page.wait_for_selector(selector, timeout=3000)
                if login_button:
                    await login_button.click()
                    typer.echo("   ğŸ”„ ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ë¨")
                    return True
            except PlaywrightTimeoutError:
                continue

        typer.echo("   âŒ ë¡œê·¸ì¸ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return False

    async def _verify_login_success(self, page: Page) -> bool:
        """ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸ í—¬í¼ ë©”ì„œë“œ"""
        try:
            await page.wait_for_function(
                """() => {
                    return window.location.href.includes('reddit.com') &&
                           !window.location.href.includes('login') &&
                           (document.querySelector('[data-testid="user-drawer-button"]') ||
                            document.querySelector('.header-user-dropdown'))
                }""",
                timeout=15000,
            )
            typer.echo("âœ… Reddit ë¡œê·¸ì¸ ì„±ê³µ!")
            return True
        except PlaywrightTimeoutError:
            current_url = page.url
            if "reddit.com" in current_url and "login" not in current_url:
                typer.echo("âœ… Reddit ë¡œê·¸ì¸ ì„±ê³µ!")
                return True
            else:
                typer.echo("âŒ Reddit ë¡œê·¸ì¸ ì‹¤íŒ¨ - URL í™•ì¸")
                if self.debug_mode:
                    await self._save_debug_html(page, "reddit_login_failed.html")
                return False

    async def _login(self, page: Page) -> bool:
        """Reddit ë¡œê·¸ì¸"""
        try:
            typer.echo("ğŸ”‘ Reddit ë¡œê·¸ì¸ ì¤‘...")
            await page.goto("https://www.reddit.com/login/", wait_until="domcontentloaded")
            await page.wait_for_timeout(3000)

            # ì‚¬ìš©ìëª… ì…ë ¥
            username_selectors = [
                "#login-username",
                "#loginUsername",
                'input[name="username"]',
                'input[placeholder*="username" i]',
                'input[type="text"]',
            ]
            if not await self._fill_login_field(
                page, username_selectors, self.username, "ì‚¬ìš©ìëª…"
            ):
                return False

            # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
            password_selectors = [
                "#login-password",
                "#loginPassword",
                'input[name="password"]',
                'input[type="password"]',
            ]
            if not await self._fill_login_field(
                page, password_selectors, self.password, "ë¹„ë°€ë²ˆí˜¸"
            ):
                return False

            # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
            if not await self._click_login_button(page):
                return False

            # ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
            return await self._verify_login_success(page)

        except Exception as e:
            typer.echo(f"âŒ ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_login_error.html")
            return False

    async def _progressive_post_collection(self, page: Page, target_count: int) -> List[Post]:
        """ì ì§„ì  ê²Œì‹œê¸€ ìˆ˜ì§‘"""
        posts = []
        scroll_attempts = 0

        typer.echo(f"ğŸ”„ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)")

        while len(posts) < target_count and scroll_attempts < self.max_scroll_attempts:
            # í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ì¶”ì¶œ
            current_posts = await self._collect_posts(page)

            # ìƒˆë¡œìš´ ê²Œì‹œê¸€ë§Œ ì¶”ê°€
            new_posts_count = 0
            for post_data in current_posts:
                if len(posts) >= target_count:
                    break

                # ì¤‘ë³µ í™•ì¸
                is_duplicate = any(
                    existing_post.url == post_data.get("url")
                    or (
                        existing_post.content == post_data.get("content")
                        and existing_post.author == post_data.get("author")
                    )
                    for existing_post in posts
                )

                if not is_duplicate:
                    post = Post(
                        platform="reddit",
                        author=post_data.get("author", "Unknown"),
                        content=post_data.get("content", ""),
                        timestamp=post_data.get("timestamp", ""),
                        url=post_data.get("url"),
                        likes=post_data.get("likes"),
                        comments=post_data.get("comments"),
                        shares=post_data.get("shares"),
                    )
                    posts.append(post)
                    new_posts_count += 1

            typer.echo(f"   ğŸ“Š ìˆ˜ì§‘ í˜„í™©: {len(posts)}/{target_count} (+{new_posts_count}ê°œ ì‹ ê·œ)")

            if len(posts) >= target_count:
                break

            if new_posts_count == 0:
                typer.echo("   âš ï¸ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŒ")

            # í˜ì´ì§€ ìŠ¤í¬ë¡¤
            await self._scroll_for_more_posts(page)
            scroll_attempts += 1

        typer.echo(f"âœ… ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return posts

    async def _collect_posts(self, page: Page) -> List[Dict[str, Any]]:
        """í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶° ê°œì„ """
        all_posts = []
        # ë‹¤ì–‘í•œ post_container ì„ íƒì
        post_selectors = [
            'div[data-testid="post-container"]',
            "div[data-ad-position]",
            "div.Post",
        ]

        post_container = None
        for selector in post_selectors:
            try:
                post_container = page.locator(selector)
                count = await post_container.count()
                if count > 0:
                    typer.echo(f"   ğŸ” '{selector}' ì„ íƒìë¡œ {count}ê°œ ê²Œì‹œê¸€ ì»¨í…Œì´ë„ˆ ë°œê²¬")
                    break
            except Exception:
                continue

        if not post_container or await post_container.count() == 0:
            typer.echo("   âŒ ê²Œì‹œê¸€ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_no_posts_found.html")
            return []

        elements = await post_container.all()

        for element in elements:
            post_data = await self._extract_post_data(element)
            if post_data:
                all_posts.append(post_data)

        return all_posts

    async def _extract_post_data(self, element) -> Optional[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            author = await self._extract_author(element)
            content = await self._extract_content(element)

            # ì €ìë‚˜ ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ìœ íš¨í•œ ê²Œì‹œê¸€ì´ ì•„ë‹˜
            if not author and not content:
                return None

            interactions = await self._extract_interactions(element)
            return {
                "author": author,
                "content": content,
                "timestamp": await self._extract_timestamp(element),
                "url": await self._extract_url(element),
                "likes": interactions.get("likes"),
                "comments": interactions.get("comments"),
                "shares": None,  # Redditì€ ê³µìœ  ìˆ˜ë¥¼ ì§ì ‘ í‘œì‹œí•˜ì§€ ì•ŠìŒ
            }
        except Exception:
            return None

    async def _extract_author(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ ì‘ì„±ì ì¶”ì¶œ"""
        author_selectors = [
            'a[data-testid="post_author_link"]',
            '[data-testid="post-meta-info"] > span:first-of-type',
            'a[href*="/user/"]',
            'span[class*="author"]',
        ]
        try:
            for selector in author_selectors:
                try:
                    author_element = await element.query_selector(selector)
                    if author_element:
                        return (await author_element.inner_text()).strip()
                except Exception:
                    continue
        except Exception:
            pass
        return "Unknown"

    async def _extract_content(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ ì½˜í…ì¸ (ì œëª©) ì¶”ì¶œ"""
        title_selectors = [
            "h3",
            "h2",
            'div[data-testid="post-title"]',
            'a[data-click-id="body"] > div > h3',
        ]
        try:
            for selector in title_selectors:
                try:
                    title_element = await element.query_selector(selector)
                    if title_element:
                        return (await title_element.inner_text()).strip()
                except Exception:
                    continue
        except Exception:
            pass
        return ""

    async def _extract_timestamp(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ"""
        timestamp_selectors = [
            'span[data-testid="post_timestamp"]',
            'a[data-testid="post_timestamp"]',
            'span[class*="timestamp"]',
        ]
        try:
            for selector in timestamp_selectors:
                try:
                    time_element = await element.query_selector(selector)
                    if time_element:
                        return (await time_element.inner_text()).strip()
                except Exception:
                    continue
        except Exception:
            pass
        return ""

    async def _extract_url(self, element) -> Optional[str]:
        """ê²Œì‹œê¸€ì—ì„œ URL ì¶”ì¶œ"""
        url_selectors = [
            'a[data-testid="post_title"]',
            'a[data-click-id="body"]',
            'a[href*="/comments/"]',
        ]
        try:
            for selector in url_selectors:
                try:
                    url_element = await element.query_selector(selector)
                    if url_element:
                        href = await url_element.get_attribute("href")
                        if href and href.startswith("/"):
                            return self.base_url + href
                        return href
                except Exception:
                    continue
        except Exception:
            pass
        return None

    async def _extract_interactions(self, element) -> Dict[str, int]:
        """ê²Œì‹œê¸€ì—ì„œ ìƒí˜¸ì‘ìš©(ì—…ë³´íŠ¸, ëŒ“ê¸€) ë°ì´í„° ì¶”ì¶œ"""
        interactions = {"likes": 0, "comments": 0}

        try:
            # ì—…ë³´íŠ¸ ì¶”ì¶œ
            upvote_text = ""
            upvote_elements = await element.query_selector_all(
                '[data-testid="post-content"] > div:last-child > div:first-child > button:first-child > span'
            )
            if upvote_elements:
                upvote_text = await upvote_elements[0].inner_text()
            else:
                # ë‹¤ë¥¸ ì„ íƒì ì‹œë„
                upvote_text_element = await element.query_selector('[id*="vote-arrows"] > div')
                if upvote_text_element:
                    upvote_text = await upvote_text_element.inner_text()

            interactions["likes"] = self._parse_number_from_text(upvote_text)

        except Exception:
            interactions["likes"] = 0

        try:
            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            comment_text = ""
            comment_element = await element.query_selector('a[data-testid="comment-button"]')
            if comment_element:
                comment_text_span = await comment_element.query_selector("span")
                if comment_text_span:
                    comment_text = await comment_text_span.inner_text()

            interactions["comments"] = self._parse_number_from_text(comment_text)

        except Exception:
            interactions["comments"] = 0

        return interactions

    def _parse_number_from_text(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì íŒŒì‹± (ì˜ˆ: '1.7k' -> 1700)"""
        if not text:
            return 0
        text = text.lower().strip()
        try:
            if "k" in text:
                num_part = text.replace("k", "").strip()
                return int(float(num_part) * 1000)
            if "m" in text:
                num_part = text.replace("m", "").strip()
                return int(float(num_part) * 1_000_000)

            # ìˆ«ìë§Œ ìˆëŠ”ì§€ í™•ì¸
            cleaned_text = re.sub(r"[^\d.]", "", text)
            if cleaned_text:
                return int(float(cleaned_text))

        except (ValueError, TypeError):
            return 0
        return 0

    async def _scroll_for_more_posts(self, page: Page):
        """ë” ë§ì€ ê²Œì‹œê¸€ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ìŠ¤í¬ë¡¤"""
        try:
            typer.echo("   ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤...")
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            # ìŠ¤í¬ë¡¤ í›„ ì½˜í…ì¸ ê°€ ë¡œë“œë  ì‹œê°„ì„ ì¤Œ
            await page.wait_for_timeout(3000)
        except Exception as e:
            typer.echo(f"   âš ï¸ ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    async def _load_session(self, page: Page) -> bool:
        """ì €ì¥ëœ ì„¸ì…˜ ë¡œë“œ"""
        if self.session_path.exists():
            typer.echo("ğŸ’¾ ì €ì¥ëœ ì„¸ì…˜ ë¡œë“œ ì¤‘...")
            try:
                await page.context.storage_state(path=self.session_path)
                # ì„¸ì…˜ ìœ íš¨ì„± ê²€ì‚¬
                await page.goto(self.base_url, wait_until="domcontentloaded")
                # ë¡œê·¸ì¸ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìš”ì†Œ í™•ì¸
                is_logged_in = await page.is_visible('[data-testid="user-drawer-button"]')
                if is_logged_in:
                    typer.echo("   âœ… ì„¸ì…˜ ìœ íš¨í•¨, ë¡œê·¸ì¸ ê±´ë„ˆëœ€")
                    return True
                else:
                    typer.echo("   âš ï¸ ì„¸ì…˜ ë§Œë£Œë¨, ì¬ë¡œê·¸ì¸ í•„ìš”")
                    return False
            except Exception:
                typer.echo("   âŒ ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨, ì¬ë¡œê·¸ì¸ í•„ìš”")
                return False
        return False

    async def _save_session(self, page: Page):
        """í˜„ì¬ ì„¸ì…˜ ì €ì¥"""
        try:
            typer.echo("ğŸ’¾ í˜„ì¬ ì„¸ì…˜ ì €ì¥ ì¤‘...")
            await page.context.storage_state(path=self.session_path)
            typer.echo("   âœ… ì„¸ì…˜ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            typer.echo(f"   âŒ ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _save_debug_html(self, page: Page, filename: str):
        """ë””ë²„ê·¸ìš© HTML íŒŒì¼ ì €ì¥"""
        if self.debug_mode:
            debug_path = Path("debug/reddit")
            debug_path.mkdir(parents=True, exist_ok=True)
            full_path = debug_path / filename
            try:
                content = await page.content()
                with open(full_path, "w", encoding="utf-8") as f:
                    f.write(content)
                typer.echo(f"   ğŸ› ë””ë²„ê·¸ HTML ì €ì¥: {full_path}")
            except Exception as e:
                typer.echo(f"   âŒ ë””ë²„ê·¸ HTML ì €ì¥ ì‹¤íŒ¨: {e}")
