"""
@file reddit.py
@description Reddit í”Œë«í¼ ì „ìš© í¬ë¡¤ëŸ¬

ì´ ëª¨ë“ˆì€ Reddit í”Œë«í¼ì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. Reddit í”¼ë“œì—ì„œ ê²Œì‹œê¸€ ìˆ˜ì§‘
2. Reddit ê³„ì •ì„ í†µí•œ ë¡œê·¸ì¸ ì§€ì›
3. ì‘ì„±ì, ì½˜í…ì¸ , ì—…ë³´íŠ¸/ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ
4. ì„¸ì…˜ ê´€ë¦¬ (ì¬ë¡œê·¸ì¸ ë°©ì§€)
5. ì ì§„ì  ì¶”ì¶œ ì‹œìŠ¤í…œ (ìŠ¤í¬ë¡¤ë§, ì„œë¸Œë ˆë”§ íƒìƒ‰)

í•µì‹¬ êµ¬í˜„ ë¡œì§:
- Reddit ë¡œê·¸ì¸ì„ í†µí•œ í”¼ë“œ ì ‘ê·¼
- ì ì§„ì  ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ë” ë§ì€ ê²Œì‹œê¸€ ë¡œë“œ
- div[data-testid="post-container"] ê¸°ë°˜ ê²Œì‹œê¸€ ì¶”ì¶œ
- K/M ë‹¨ìœ„ ì—…ë³´íŠ¸ ìˆ˜ì¹˜ íŒŒì‹±

@dependencies
- playwright.async_api: ë¸Œë¼ìš°ì € ìë™í™”
- typer: CLI ì¶œë ¥
- .base: ë² ì´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤

@see {@link https://www.reddit.com} - Reddit í”Œë«í¼
"""

import os
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import typer
from dotenv import load_dotenv
from playwright.async_api import Page
from playwright.async_api import TimeoutError as PlaywrightTimeoutError

from src.crawlers.base import BaseCrawler
from src.models import Post

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class RedditCrawler(BaseCrawler):
    """Reddit ì „ìš© í¬ë¡¤ëŸ¬"""

    def __init__(self, debug_mode: bool = False):
        super().__init__(
            platform_name="reddit", base_url="https://www.reddit.com", debug_mode=debug_mode
        )
        self.username = os.getenv("REDDIT_USERNAME")
        self.password = os.getenv("REDDIT_PASSWORD")
        self.session_path = Path("data/sessions/reddit_session.json")
        self.max_scroll_attempts = 10

        if not self.username or not self.password:
            raise ValueError("REDDIT_USERNAMEê³¼ REDDIT_PASSWORD í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    async def _crawl_implementation(self, page: Page, count: int) -> List[Post]:
        """Reddit ê²Œì‹œê¸€ í¬ë¡¤ë§ êµ¬í˜„"""
        try:
            # ì„¸ì…˜ ë¡œë“œ ì‹œë„
            if not await self._load_session(page):
                # ì„¸ì…˜ì´ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ë¡œê·¸ì¸
                await self._login(page)
                await self._save_session(page)

            # ë©”ì¸ í”¼ë“œë¡œ ì´ë™
            await page.goto("https://www.reddit.com/", wait_until="domcontentloaded")
            await page.wait_for_timeout(3000)

            # ê²Œì‹œê¸€ ìˆ˜ì§‘
            posts = await self._progressive_post_collection(page, count)

            if self.debug_mode:
                await self._save_debug_html(page, "reddit_posts.html")

            return posts

        except Exception as e:
            typer.echo(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_error.html")
            return []

    async def crawl(self, count: int = 10) -> List[Post]:
        """Reddit ê²Œì‹œê¸€ í¬ë¡¤ë§ - ë² ì´ìŠ¤ í´ë˜ìŠ¤ ì˜¤ë²„ë¼ì´ë“œ"""
        typer.echo(f"ğŸ”´ Reddit í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {count}ê°œ)")
        return await super().crawl(count)

    async def _login(self, page: Page) -> bool:
        """Reddit ë¡œê·¸ì¸"""
        try:
            typer.echo("ğŸ”‘ Reddit ë¡œê·¸ì¸ ì¤‘...")
            await page.goto("https://www.reddit.com/login/", wait_until="domcontentloaded")

            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            await page.wait_for_timeout(2000)

            typer.echo("   - ì‚¬ìš©ìëª… ì…ë ¥ ì‹œë„...")
            username_input = page.get_by_role("textbox", name="Email or username")
            await username_input.fill(self.username)
            typer.echo(f"   âœ… ì‚¬ìš©ìëª… ì…ë ¥ ì™„ë£Œ: {self.username}")

            typer.echo("   - ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì‹œë„...")
            password_input = page.get_by_role("textbox", name="Password")
            await password_input.fill(self.password)
            typer.echo("   âœ… ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì™„ë£Œ")

            typer.echo("   - ë¡œê·¸ì¸ ë²„íŠ¼ í™œì„±í™” ëŒ€ê¸°...")
            login_button = page.get_by_role("button", name="Log In")

            # ë¡œê·¸ì¸ ë²„íŠ¼ì´ í™œì„±í™”ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸°
            await login_button.wait_for(state="visible", timeout=5000)

            # ë²„íŠ¼ì´ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            for _ in range(10):  # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
                is_enabled = await login_button.is_enabled()
                if is_enabled:
                    break
                await page.wait_for_timeout(500)

            if not await login_button.is_enabled():
                typer.echo("   âŒ ë¡œê·¸ì¸ ë²„íŠ¼ì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
                if self.debug_mode:
                    await self._save_debug_html(page, "reddit_login_button_disabled.html")
                return False

            typer.echo("   - ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì‹œë„...")
            await login_button.click()
            typer.echo("   ğŸ”„ ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ë¨")

            typer.echo("   - ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸ ì¤‘...")
            try:
                # ì„±ê³µ: ë©”ì¸ í˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤ (ìµœëŒ€ 15ì´ˆ).
                await page.wait_for_url("https://www.reddit.com/", timeout=15000)
                typer.echo("âœ… Reddit ë¡œê·¸ì¸ ì„±ê³µ!")
                return True
            except PlaywrightTimeoutError:
                # ì‹¤íŒ¨: ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
                error_message = page.locator('text="Invalid username or password."')
                if await error_message.is_visible():
                    typer.echo("âŒ Reddit ë¡œê·¸ì¸ ì‹¤íŒ¨: ì˜ëª»ëœ ì‚¬ìš©ì ì´ë¦„ ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸")
                else:
                    typer.echo("âŒ Reddit ë¡œê·¸ì¸ ì‹¤íŒ¨: ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                if self.debug_mode:
                    await self._save_debug_html(page, "reddit_login_failed.html")
                return False

        except Exception as e:
            typer.echo(f"âŒ ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_login_error.html")
            return False

    async def _progressive_post_collection(self, page: Page, target_count: int) -> List[Post]:
        """ì ì§„ì  ê²Œì‹œê¸€ ìˆ˜ì§‘"""
        posts = []
        scroll_attempts = 0

        typer.echo(f"ğŸ”„ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)")

        while len(posts) < target_count and scroll_attempts < self.max_scroll_attempts:
            # í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ì¶”ì¶œ
            current_posts = await self._collect_posts(page)

            # ìƒˆë¡œìš´ ê²Œì‹œê¸€ë§Œ ì¶”ê°€
            new_posts_count = 0
            for post_data in current_posts:
                if len(posts) >= target_count:
                    break

                # ì¤‘ë³µ í™•ì¸
                is_duplicate = any(
                    existing_post.url == post_data.get("url")
                    or (
                        existing_post.content == post_data.get("content")
                        and existing_post.author == post_data.get("author")
                    )
                    for existing_post in posts
                )

                if not is_duplicate:
                    post = Post(
                        platform="reddit",
                        author=post_data.get("author", "Unknown"),
                        content=post_data.get("content", ""),
                        timestamp=post_data.get("timestamp", ""),
                        url=post_data.get("url"),
                        likes=post_data.get("likes"),
                        comments=post_data.get("comments"),
                        shares=post_data.get("shares"),
                    )
                    posts.append(post)
                    new_posts_count += 1

            typer.echo(f"   ğŸ“Š ìˆ˜ì§‘ í˜„í™©: {len(posts)}/{target_count} (+{new_posts_count}ê°œ ì‹ ê·œ)")

            if len(posts) >= target_count:
                break

            if new_posts_count == 0:
                typer.echo("   âš ï¸ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŒ")

            # í˜ì´ì§€ ìŠ¤í¬ë¡¤
            await self._scroll_for_more_posts(page)
            scroll_attempts += 1

        typer.echo(f"âœ… ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return posts

    async def _collect_posts(self, page: Page) -> List[Dict[str, Any]]:
        """í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶° ê°œì„ """
        all_posts = []

        # ì‹¤ì œ Reddit êµ¬ì¡°ì— ë”°ë¼ article íƒœê·¸ ì‚¬ìš©
        try:
            post_containers = page.locator("article")
            count = await post_containers.count()

            if count > 0:
                typer.echo(f"   ğŸ” {count}ê°œ ê²Œì‹œê¸€ (article) ë°œê²¬")

                elements = await post_containers.all()

                for element in elements:
                    post_data = await self._extract_post_data(element)
                    if post_data:
                        all_posts.append(post_data)

            else:
                typer.echo("   âŒ ê²Œì‹œê¸€ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                if self.debug_mode:
                    await self._save_debug_html(page, "reddit_no_posts_found.html")

        except Exception as e:
            typer.echo(f"   âŒ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_collection_error.html")

        return all_posts

    async def _extract_post_data(self, element) -> Optional[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            author = await self._extract_author(element)
            content = await self._extract_content(element)

            # ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
            if self.debug_mode:
                typer.echo(f"   ğŸ” ì¶”ì¶œ ë°ì´í„°: author='{author}', content='{content[:50]}...'")

            # ì €ìë‚˜ ì½˜í…ì¸  ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ìœ íš¨í•œ ê²Œì‹œê¸€ë¡œ íŒë‹¨ (ì¡°ê±´ ì™„í™”)
            if not author and not content:
                if self.debug_mode:
                    typer.echo("   âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ê²Œì‹œê¸€ ê±´ë„ˆëœ€")
                return None

            interactions = await self._extract_interactions(element)

            post_data = {
                "author": author or "Unknown",
                "content": content or "No title",
                "timestamp": await self._extract_timestamp(element),
                "url": await self._extract_url(element),
                "likes": interactions.get("likes", 0),
                "comments": interactions.get("comments", 0),
                "shares": None,  # Redditì€ ê³µìœ  ìˆ˜ë¥¼ ì§ì ‘ í‘œì‹œí•˜ì§€ ì•ŠìŒ
            }

            if self.debug_mode:
                typer.echo(
                    f"   âœ… ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {post_data['author']} - {post_data['content'][:30]}..."
                )

            return post_data
        except Exception as e:
            if self.debug_mode:
                typer.echo(f"   âŒ ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def _extract_author(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ ì‘ì„±ì ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶˜ ê°œì„ """
        try:
            # Reddit êµ¬ì¡°ì—ì„œ ì„œë¸Œë ˆë”§ ì •ë³´ ì¶”ì¶œ (r/subreddit í˜•íƒœ)
            subreddit_selectors = [
                'a[href*="/r/"]',  # r/subreddit ë§í¬
                'link[href*="/r/"]',  # generic link í˜•íƒœ
            ]

            for selector in subreddit_selectors:
                try:
                    subreddit_element = await element.query_selector(selector)
                    if subreddit_element:
                        href = await subreddit_element.get_attribute("href")
                        if href and "/r/" in href:
                            # /r/subreddit í˜•íƒœì—ì„œ ì„œë¸Œë ˆë”§ëª… ì¶”ì¶œ
                            subreddit_name = href.split("/r/")[-1].split("/")[0]
                            if subreddit_name:
                                return f"r/{subreddit_name}"
                except Exception:
                    continue

            # ì„œë¸Œë ˆë”§ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì°¾ê¸°
            text_content = await element.inner_text()
            if "r/" in text_content:
                subreddit_match = re.search(r"r/([a-zA-Z0-9_]+)", text_content)
                if subreddit_match:
                    return f"r/{subreddit_match.group(1)}"

        except Exception:
            pass
        return "Unknown"

    async def _extract_content(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ ì½˜í…ì¸ (ì œëª©) ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶˜ ê°œì„ """
        try:
            # 1. article íƒœê·¸ì˜ aria-label ì†ì„± í™•ì¸ (ê°€ì¥ ì •í™•í•œ ë°©ë²•)
            try:
                article_aria_label = await element.get_attribute("aria-label")
                if article_aria_label and len(article_aria_label) > 3:
                    return article_aria_label.strip()
            except Exception:
                pass

            # 2. Reddit êµ¬ì¡°ì—ì„œ ì œëª©ì€ heading íƒœê·¸ì— ìˆìŒ (level=2)
            title_selectors = [
                'heading[level="2"]',  # ì •í™•í•œ heading íƒœê·¸
                "h2",  # ì¼ë°˜ì ì¸ h2 íƒœê·¸
                "h3",  # ëŒ€ì²´ìš© h3 íƒœê·¸
            ]

            for selector in title_selectors:
                try:
                    title_element = await element.query_selector(selector)
                    if title_element:
                        title_text = (await title_element.inner_text()).strip()
                        if title_text and len(title_text) > 3:
                            return title_text
                except Exception:
                    continue

            # 3. ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
            link_selectors = [
                'a[href*="/comments/"]',  # ëŒ“ê¸€ ë§í¬
                'a[href*="/r/"]',  # ì„œë¸Œë ˆë”§ ë§í¬
            ]

            for selector in link_selectors:
                try:
                    link_element = await element.query_selector(selector)
                    if link_element:
                        link_text = (await link_element.inner_text()).strip()
                        if link_text and not link_text.startswith("r/") and len(link_text) > 3:
                            return link_text
                except Exception:
                    continue

        except Exception:
            pass
        return ""

    async def _extract_timestamp(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶˜ ê°œì„ """
        try:
            # Reddit êµ¬ì¡°ì—ì„œ ì‹œê°„ ì •ë³´ëŠ” time íƒœê·¸ì— ìˆìŒ
            timestamp_selectors = [
                "time",  # ì‹¤ì œ time íƒœê·¸
                'span[class*="timestamp"]',  # ëŒ€ì²´ìš© ì„ íƒì
                'span[class*="time"]',  # ì‹œê°„ ê´€ë ¨ span
            ]

            for selector in timestamp_selectors:
                try:
                    time_element = await element.query_selector(selector)
                    if time_element:
                        time_text = (await time_element.inner_text()).strip()
                        if time_text:
                            return time_text
                except Exception:
                    continue

            # ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ì°¾ê¸°
            text_content = await element.inner_text()
            if text_content:
                # "X hr. ago", "X min. ago", "X days ago" ë“±ì˜ íŒ¨í„´ ì°¾ê¸°
                time_patterns = [
                    r"(\d+)\s+(hr|hour|hours)\.?\s+ago",
                    r"(\d+)\s+(min|minute|minutes)\.?\s+ago",
                    r"(\d+)\s+(day|days)\.?\s+ago",
                    r"(\d+)\s+(sec|second|seconds)\.?\s+ago",
                ]

                for pattern in time_patterns:
                    match = re.search(pattern, text_content, re.IGNORECASE)
                    if match:
                        return match.group(0)

        except Exception:
            pass
        return ""

    async def _extract_url(self, element) -> Optional[str]:
        """ê²Œì‹œê¸€ì—ì„œ URL ì¶”ì¶œ"""
        url_selectors = [
            'a[data-testid="post_title"]',
            'a[data-click-id="body"]',
            'a[href*="/comments/"]',
        ]
        try:
            for selector in url_selectors:
                try:
                    url_element = await element.query_selector(selector)
                    if url_element:
                        href = await url_element.get_attribute("href")
                        if href and href.startswith("/"):
                            return self.base_url + href
                        return href
                except Exception:
                    continue
        except Exception:
            pass
        return None

    async def _extract_interactions(self, element) -> Dict[str, int]:
        """ê²Œì‹œê¸€ì—ì„œ ìƒí˜¸ì‘ìš©(ì—…ë³´íŠ¸, ëŒ“ê¸€) ë°ì´í„° ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶˜ ê°œì„ """
        interactions = {"likes": 0, "comments": 0}

        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            text_content = await element.inner_text()
            if text_content:
                # 1. ì—…ë³´íŠ¸ ìˆ˜ ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì—ì„œ íŒ¨í„´ ì°¾ê¸°
                upvote_patterns = [
                    r"Upvote\s+(\d+\.?\d*[KM]?)\s+Downvote",  # "Upvote 307 Downvote"
                    r"generic:\s*\"(\d+\.?\d*[KM]?)\"\s+.*Downvote",  # "generic: "307" ... Downvote"
                    r"(\d+\.?\d*[KM]?)\s+Go to comments",  # ë•Œë¡œëŠ” ì—…ë³´íŠ¸ ìˆ˜ê°€ ëŒ“ê¸€ ì „ì— ë‚˜íƒ€ë‚¨
                ]

                for pattern in upvote_patterns:
                    match = re.search(pattern, text_content)
                    if match:
                        interactions["likes"] = self._parse_number_from_text(match.group(1))
                        break

                # 2. ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ - "Go to comments" ë§í¬ì—ì„œ ìˆ«ì ì°¾ê¸°
                comment_patterns = [
                    r"(\d+\.?\d*[KM]?)\s+Go to comments",  # "67 Go to comments"
                    r"link\s+\"(\d+\.?\d*[KM]?)\s+Go to comments\"",  # ë§í¬ ë‚´ í…ìŠ¤íŠ¸
                ]

                for pattern in comment_patterns:
                    match = re.search(pattern, text_content)
                    if match:
                        interactions["comments"] = self._parse_number_from_text(match.group(1))
                        break

                # 3. ëŒ€ì²´ ë°©ë²• - ê° ë¼ì¸ì—ì„œ ìˆ«ì ì°¾ê¸°
                if interactions["likes"] == 0 or interactions["comments"] == 0:
                    lines = text_content.split("\n")
                    for i, line in enumerate(lines):
                        line = line.strip()

                        # ì—…ë³´íŠ¸ ìˆ˜ ì°¾ê¸°
                        if interactions["likes"] == 0:
                            if "Upvote" in line and "Downvote" in line:
                                numbers = re.findall(r"(\d+\.?\d*[KM]?)", line)
                                if numbers:
                                    interactions["likes"] = self._parse_number_from_text(numbers[0])
                            elif line.isdigit() or (re.match(r"^\d+\.?\d*[KM]?$", line)):
                                # ë‹¤ìŒ ë¼ì¸ì´ Downvoteì¸ì§€ í™•ì¸
                                next_line = lines[i + 1] if i + 1 < len(lines) else ""
                                if "Downvote" in next_line:
                                    interactions["likes"] = self._parse_number_from_text(line)

                        # ëŒ“ê¸€ ìˆ˜ ì°¾ê¸°
                        if interactions["comments"] == 0 and "Go to comments" in line:
                            numbers = re.findall(r"(\d+\.?\d*[KM]?)", line)
                            if numbers:
                                interactions["comments"] = self._parse_number_from_text(numbers[0])

        except Exception:
            pass

        return interactions

    def _parse_number_from_text(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì íŒŒì‹± (ì˜ˆ: '1.7k' -> 1700)"""
        if not text:
            return 0
        text = text.lower().strip()
        try:
            if "k" in text:
                num_part = text.replace("k", "").strip()
                return int(float(num_part) * 1000)
            if "m" in text:
                num_part = text.replace("m", "").strip()
                return int(float(num_part) * 1_000_000)

            # ìˆ«ìë§Œ ìˆëŠ”ì§€ í™•ì¸
            cleaned_text = re.sub(r"[^\d.]", "", text)
            if cleaned_text:
                return int(float(cleaned_text))

        except (ValueError, TypeError):
            return 0
        return 0

    async def _scroll_for_more_posts(self, page: Page):
        """ë” ë§ì€ ê²Œì‹œê¸€ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ìŠ¤í¬ë¡¤"""
        try:
            typer.echo("   ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤...")
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            # ìŠ¤í¬ë¡¤ í›„ ì½˜í…ì¸ ê°€ ë¡œë“œë  ì‹œê°„ì„ ì¤Œ
            await page.wait_for_timeout(3000)
        except Exception as e:
            typer.echo(f"   âš ï¸ ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    async def _load_session(self, page: Page) -> bool:
        """ì €ì¥ëœ ì„¸ì…˜ ë¡œë“œ"""
        if self.session_path.exists():
            typer.echo("ğŸ’¾ ì €ì¥ëœ ì„¸ì…˜ ë¡œë“œ ì¤‘...")
            try:
                await page.context.storage_state(path=self.session_path)
                # ì„¸ì…˜ ìœ íš¨ì„± ê²€ì‚¬
                await page.goto(self.base_url, wait_until="domcontentloaded")
                # ë¡œê·¸ì¸ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìš”ì†Œ í™•ì¸
                is_logged_in = await page.is_visible('[data-testid="user-drawer-button"]')
                if is_logged_in:
                    typer.echo("   âœ… ì„¸ì…˜ ìœ íš¨í•¨, ë¡œê·¸ì¸ ê±´ë„ˆëœ€")
                    return True
                else:
                    typer.echo("   âš ï¸ ì„¸ì…˜ ë§Œë£Œë¨, ì¬ë¡œê·¸ì¸ í•„ìš”")
                    return False
            except Exception:
                typer.echo("   âŒ ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨, ì¬ë¡œê·¸ì¸ í•„ìš”")
                return False
        return False

    async def _save_session(self, page: Page):
        """í˜„ì¬ ì„¸ì…˜ ì €ì¥"""
        try:
            typer.echo("ğŸ’¾ í˜„ì¬ ì„¸ì…˜ ì €ì¥ ì¤‘...")
            await page.context.storage_state(path=self.session_path)
            typer.echo("   âœ… ì„¸ì…˜ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            typer.echo(f"   âŒ ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _save_debug_html(self, page: Page, filename: str):
        """ë””ë²„ê·¸ìš© HTML íŒŒì¼ ì €ì¥"""
        if self.debug_mode:
            debug_path = Path("debug/reddit")
            debug_path.mkdir(parents=True, exist_ok=True)
            full_path = debug_path / filename
            try:
                content = await page.content()
                with open(full_path, "w", encoding="utf-8") as f:
                    f.write(content)
                typer.echo(f"   ğŸ› ë””ë²„ê·¸ HTML ì €ì¥: {full_path}")
            except Exception as e:
                typer.echo(f"   âŒ ë””ë²„ê·¸ HTML ì €ì¥ ì‹¤íŒ¨: {e}")
