"""
@file reddit.py
@description Reddit í”Œë«í¼ ì „ìš© í¬ë¡¤ëŸ¬

ì´ ëª¨ë“ˆì€ Reddit í”Œë«í¼ì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. Reddit í”¼ë“œì—ì„œ ê²Œì‹œê¸€ ìˆ˜ì§‘
2. Reddit ê³„ì •ì„ í†µí•œ ë¡œê·¸ì¸ ì§€ì›
3. ì‘ì„±ì, ì½˜í…ì¸ , ì—…ë³´íŠ¸/ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ
4. ì„¸ì…˜ ê´€ë¦¬ (ì¬ë¡œê·¸ì¸ ë°©ì§€)
5. ì ì§„ì  ì¶”ì¶œ ì‹œìŠ¤í…œ (ìŠ¤í¬ë¡¤ë§, ì„œë¸Œë ˆë”§ íƒìƒ‰)

í•µì‹¬ êµ¬í˜„ ë¡œì§:
- Reddit ë¡œê·¸ì¸ì„ í†µí•œ í”¼ë“œ ì ‘ê·¼
- ì ì§„ì  ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ë” ë§ì€ ê²Œì‹œê¸€ ë¡œë“œ
- div[data-testid="post-container"] ê¸°ë°˜ ê²Œì‹œê¸€ ì¶”ì¶œ
- K/M ë‹¨ìœ„ ì—…ë³´íŠ¸ ìˆ˜ì¹˜ íŒŒì‹±

@dependencies
- playwright.async_api: ë¸Œë¼ìš°ì € ìë™í™”
- typer: CLI ì¶œë ¥
- .base: ë² ì´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤

@see {@link https://www.reddit.com} - Reddit í”Œë«í¼
"""

import json
import os
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import typer
from dotenv import load_dotenv
from playwright.async_api import Page

from src.crawlers.base import BaseCrawler
from src.models import Post

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class RedditCrawler(BaseCrawler):
    """Reddit ì „ìš© í¬ë¡¤ëŸ¬"""

    def __init__(self, debug_mode: bool = False):
        super().__init__(
            platform_name="reddit", base_url="https://www.reddit.com", debug_mode=debug_mode
        )
        self.username = os.getenv("REDDIT_USERNAME")
        self.password = os.getenv("REDDIT_PASSWORD")
        self.session_path = Path("data/sessions/reddit_session.json")
        self.max_scroll_attempts = 10

        if not self.username or not self.password:
            raise ValueError("REDDIT_USERNAMEê³¼ REDDIT_PASSWORD í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    async def _crawl_implementation(self, page: Page, count: int) -> List[Post]:
        """Reddit ê²Œì‹œê¸€ í¬ë¡¤ë§ êµ¬í˜„"""
        try:
            # ì„¸ì…˜ ë¡œë“œ ì‹œë„
            login_success = await self._load_session(page)

            if not login_success:
                # ì„¸ì…˜ì´ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ë¡œê·¸ì¸
                login_success = await self._login(page)

                if not login_success:
                    typer.echo("âŒ Reddit ë¡œê·¸ì¸ ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    return []

                # ë¡œê·¸ì¸ ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì„¸ì…˜ ì €ì¥
                await self._save_session(page)

            # ë©”ì¸ í”¼ë“œë¡œ ì´ë™
            await page.goto("https://www.reddit.com/", wait_until="domcontentloaded")
            await page.wait_for_timeout(3000)

            # ê²Œì‹œê¸€ ìˆ˜ì§‘
            posts = await self._progressive_post_collection(page, count)

            if self.debug_mode:
                await self._save_debug_html(page, "reddit_posts.html")

            return posts

        except Exception as e:
            typer.echo(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_error.html")
            return []

    async def crawl(self, count: int = 10) -> List[Post]:
        """Reddit ê²Œì‹œê¸€ í¬ë¡¤ë§ - ë² ì´ìŠ¤ í´ë˜ìŠ¤ ì˜¤ë²„ë¼ì´ë“œ"""
        typer.echo(f"ğŸ”´ Reddit í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {count}ê°œ)")
        return await super().crawl(count)

    async def _login(self, page: Page) -> bool:
        """Reddit ë¡œê·¸ì¸"""
        try:
            typer.echo("ğŸ”‘ Reddit ë¡œê·¸ì¸ ì¤‘...")
            await page.goto("https://www.reddit.com/login/", wait_until="domcontentloaded")
            await page.wait_for_timeout(3000)

            # 1. ë¡œê·¸ì¸ í¼ ì…ë ¥
            if not await self._fill_login_form(page):
                return False

            # 2. ë¡œê·¸ì¸ ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
            login_button = await self._find_login_button(page)
            if not login_button:
                return False

            if not await self._wait_and_click_login_button(page, login_button):
                return False

            # 3. ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
            return await self._verify_login_success(page)

        except Exception as e:
            typer.echo(f"âŒ ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_login_exception.html")
            return False

    async def _fill_login_form(self, page: Page) -> bool:
        """ë¡œê·¸ì¸ í¼ ì…ë ¥"""
        try:
            typer.echo("   - ì‚¬ìš©ìëª… ì…ë ¥ ì‹œë„...")
            username_input = page.locator(
                'input#login-username, input[name="username"], input[id="loginUsername"]'
            ).first
            await username_input.wait_for(state="visible", timeout=5000)
            await username_input.fill(self.username)
            typer.echo(f"   âœ… ì‚¬ìš©ìëª… ì…ë ¥ ì™„ë£Œ: {self.username}")

            typer.echo("   - ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì‹œë„...")
            password_input = page.locator(
                'input#login-password, input[name="password"], input[id="loginPassword"]'
            ).first
            await password_input.wait_for(state="visible", timeout=5000)
            await password_input.fill(self.password)
            typer.echo("   âœ… ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì™„ë£Œ")

            # ì…ë ¥ í•„ë“œ ë³€ê²½ ì´ë²¤íŠ¸ íŠ¸ë¦¬ê±°
            await username_input.press("Tab")
            await password_input.press("Tab")
            await page.wait_for_timeout(500)
            return True

        except Exception as e:
            typer.echo(f"   âŒ ë¡œê·¸ì¸ í¼ ì…ë ¥ ì‹¤íŒ¨: {e}")
            return False

    async def _find_login_button(self, page: Page):
        """ë¡œê·¸ì¸ ë²„íŠ¼ ì°¾ê¸°"""
        typer.echo("   - ë¡œê·¸ì¸ ë²„íŠ¼ ì°¾ê¸°...")
        login_button_selectors = [
            'button:has-text("Log in")',
            'button:has-text("LOG IN")',
            'button:has-text("Sign in")',
            'button[type="submit"]',
            'fieldset button[class*="button"]',
            'fieldset button[class*="AnimatedForm"]',
            'button[class*="AnimatedForm__submitButton"]',
        ]

        for selector in login_button_selectors:
            try:
                btn = page.locator(selector).first
                if await btn.is_visible():
                    typer.echo(f"   âœ… ë¡œê·¸ì¸ ë²„íŠ¼ ì°¾ìŒ: {selector}")
                    return btn
            except Exception:
                continue

        typer.echo("   âŒ ë¡œê·¸ì¸ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        if self.debug_mode:
            await self._save_debug_html(page, "reddit_no_login_button.html")
        return None

    async def _wait_and_click_login_button(self, page: Page, login_button) -> bool:
        """ë¡œê·¸ì¸ ë²„íŠ¼ í™œì„±í™” ëŒ€ê¸° ë° í´ë¦­"""
        password_input = page.locator(
            'input#login-password, input[name="password"], input[id="loginPassword"]'
        ).first

        # ë²„íŠ¼ì´ í™œì„±í™”ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸°
        for i in range(10):  # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
            is_enabled = await login_button.is_enabled()
            if is_enabled:
                break
            await page.wait_for_timeout(500)
            if i == 4:  # 2.5ì´ˆ í›„ ë‹¤ì‹œ ì…ë ¥ ì‹œë„
                await password_input.press("Tab")

        if not await login_button.is_enabled():
            typer.echo("   âŒ ë¡œê·¸ì¸ ë²„íŠ¼ì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_login_button_disabled.html")
            return False

        typer.echo("   - ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ ì‹œë„...")
        await login_button.click()
        typer.echo("   ğŸ”„ ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­ë¨")
        return True

    async def _verify_login_success(self, page: Page) -> bool:
        """ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸"""
        typer.echo("   - ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸ ì¤‘...")

        for _ in range(30):  # ìµœëŒ€ 15ì´ˆ ëŒ€ê¸°
            # ë©”ì¸ í˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜ í™•ì¸
            current_url = page.url
            if current_url in ["https://www.reddit.com/", "https://www.reddit.com"]:
                typer.echo("âœ… Reddit ë¡œê·¸ì¸ ì„±ê³µ!")
                return True

            # ë¡œê·¸ì¸ëœ ìƒíƒœ í™•ì¸ (ì‚¬ìš©ì ë©”ë‰´ ë²„íŠ¼)
            if await self._check_user_menu(page):
                typer.echo("âœ… Reddit ë¡œê·¸ì¸ ì„±ê³µ! (ì‚¬ìš©ì ë©”ë‰´ í™•ì¸)")
                return True

            # ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸
            error_msg = await self._check_login_error(page)
            if error_msg:
                typer.echo(f"âŒ Reddit ë¡œê·¸ì¸ ì‹¤íŒ¨: {error_msg}")
                return False

            await page.wait_for_timeout(500)

        typer.echo("âŒ Reddit ë¡œê·¸ì¸ ì‹¤íŒ¨: íƒ€ì„ì•„ì›ƒ")
        if self.debug_mode:
            await self._save_debug_html(page, "reddit_login_timeout.html")
        return False

    async def _check_user_menu(self, page: Page) -> bool:
        """ì‚¬ìš©ì ë©”ë‰´ í™•ì¸"""
        try:
            user_menu = page.locator(
                'button[aria-label*="Expand user menu"], button[id*="USER_DROPDOWN"], div[class*="header-user-dropdown"]'
            ).first
            return await user_menu.is_visible()
        except Exception:
            return False

    async def _check_login_error(self, page: Page) -> Optional[str]:
        """ë¡œê·¸ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸"""
        error_selectors = [
            'div[class*="error"]',
            'span[class*="error"]',
            'div[class*="AnimatedForm__errorMessage"]',
            ".status-error",
            '[class*="status"][class*="error"]',
        ]

        try:
            for error_selector in error_selectors:
                error_element = page.locator(error_selector).first
                if await error_element.is_visible():
                    error_text = await error_element.inner_text()
                    if self.debug_mode:
                        await self._save_debug_html(page, "reddit_login_error.html")
                    return error_text
        except Exception:
            pass
        return None

    async def _progressive_post_collection(self, page: Page, target_count: int) -> List[Post]:
        """ì ì§„ì  ê²Œì‹œê¸€ ìˆ˜ì§‘"""
        posts = []
        scroll_attempts = 0

        typer.echo(f"ğŸ”„ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)")

        while len(posts) < target_count and scroll_attempts < self.max_scroll_attempts:
            # í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ì¶”ì¶œ
            current_posts = await self._collect_posts(page)

            # ìƒˆë¡œìš´ ê²Œì‹œê¸€ë§Œ ì¶”ê°€
            new_posts_count = 0
            for post_data in current_posts:
                if len(posts) >= target_count:
                    break

                # ì¤‘ë³µ í™•ì¸
                is_duplicate = any(
                    existing_post.url == post_data.get("url")
                    or (
                        existing_post.content == post_data.get("content")
                        and existing_post.author == post_data.get("author")
                    )
                    for existing_post in posts
                )

                if not is_duplicate:
                    post = Post(
                        platform="reddit",
                        author=post_data.get("author", "Unknown"),
                        content=post_data.get("content", ""),
                        timestamp=post_data.get("timestamp", ""),
                        url=post_data.get("url"),
                        likes=post_data.get("likes"),
                        comments=post_data.get("comments"),
                        shares=post_data.get("shares"),
                    )
                    posts.append(post)
                    new_posts_count += 1

            typer.echo(f"   ğŸ“Š ìˆ˜ì§‘ í˜„í™©: {len(posts)}/{target_count} (+{new_posts_count}ê°œ ì‹ ê·œ)")

            if len(posts) >= target_count:
                break

            if new_posts_count == 0:
                typer.echo("   âš ï¸ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŒ")

            # í˜ì´ì§€ ìŠ¤í¬ë¡¤
            await self._scroll_for_more_posts(page)
            scroll_attempts += 1

        typer.echo(f"âœ… ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return posts

    async def _collect_posts(self, page: Page) -> List[Dict[str, Any]]:
        """í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ìˆ˜ì§‘ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶° ê°œì„ """
        all_posts = []

        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        post_selectors = [
            "shreddit-post",  # ìƒˆë¡œìš´ Reddit ì›¹ ì»´í¬ë„ŒíŠ¸
            "article",  # ì¼ë°˜ article íƒœê·¸ (shreddit-postê°€ ê°ì‹¸ê³  ìˆì„ ìˆ˜ ìˆìŒ)
            'div[data-testid="post-container"]',  # ì´ì „ Reddit êµ¬ì¡°
            'div[id^="t3_"]',  # ê²Œì‹œê¸€ ID íŒ¨í„´
            'div[class*="Post"]',  # í´ë˜ìŠ¤ ê¸°ë°˜
            '[slot="post-container"]',  # slot ì†ì„± ê¸°ë°˜
        ]

        posts_found = False

        for selector in post_selectors:
            try:
                post_containers = page.locator(selector)
                count = await post_containers.count()

                if count > 0:
                    typer.echo(f"   ğŸ” {count}ê°œ ê²Œì‹œê¸€ ë°œê²¬ (ì„ íƒì: {selector})")
                    posts_found = True

                    elements = await post_containers.all()

                    for i, element in enumerate(elements):
                        if self.debug_mode and i == 0:
                            typer.echo("   ğŸ” ì²« ë²ˆì§¸ ê²Œì‹œê¸€ êµ¬ì¡° ë¶„ì„...")

                        # shreddit-post ìš”ì†Œì¸ ê²½ìš°
                        if "shreddit-post" in selector:
                            post_data = await self._extract_post_data_from_shreddit(element)
                        elif selector == "article":
                            # article íƒœê·¸ì¸ ê²½ìš° ìƒˆë¡œìš´ ì¶”ì¶œ ë°©ë²• ì‹œë„
                            post_data = await self._extract_post_data_from_article(element)
                        else:
                            # ë‹¤ë¥¸ ì„ íƒìì˜ ê²½ìš° ê¸°ì¡´ ì¶”ì¶œ ë°©ë²• ì‚¬ìš©
                            post_data = await self._extract_post_data(element)

                        if post_data:
                            all_posts.append(post_data)
                        elif self.debug_mode:
                            typer.echo(f"   âš ï¸ {i+1}ë²ˆì§¸ ê²Œì‹œê¸€ ì¶”ì¶œ ì‹¤íŒ¨")

                    if all_posts:
                        break  # ê²Œì‹œê¸€ì„ ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ ì„ íƒì ì‹œë„í•˜ì§€ ì•ŠìŒ

            except Exception as e:
                if self.debug_mode:
                    typer.echo(f"   âŒ ì„ íƒì {selector} ì‚¬ìš© ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        if not posts_found:
            typer.echo("   âŒ ì–´ë–¤ ì„ íƒìë¡œë„ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_no_posts_found.html")

                # í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
                typer.echo("   ğŸ” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì¤‘...")
                try:
                    # ê°€ëŠ¥í•œ ê²Œì‹œê¸€ ìš”ì†Œ ì°¾ê¸°
                    possible_posts = await page.evaluate(
                        """
                        () => {
                            const selectors = ['article', 'div[id^="t3_"]', '[data-testid*="post"]', 'shreddit-post'];
                            const results = {};

                            selectors.forEach(sel => {
                                const elements = document.querySelectorAll(sel);
                                if (elements.length > 0) {
                                    results[sel] = {
                                        count: elements.length,
                                        firstElement: {
                                            tagName: elements[0].tagName,
                                            className: elements[0].className,
                                            id: elements[0].id,
                                            attributes: Array.from(elements[0].attributes).map(a => a.name)
                                        }
                                    };
                                }
                            });

                            return results;
                        }
                    """
                    )
                    typer.echo(f"      ë°œê²¬ëœ ìš”ì†Œë“¤: {json.dumps(possible_posts, indent=2)}")
                except Exception:
                    pass

        return all_posts

    async def _extract_post_data_from_article(self, element) -> Optional[Dict[str, Any]]:
        """article ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ (Redditì˜ ìƒˆë¡œìš´ êµ¬ì¡°)"""
        try:
            # 1. ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ
            title = await self._extract_title_from_element(element)
            subreddit = await self._extract_subreddit_from_element(element)
            url = await self._extract_url_from_element(element)
            timestamp = await self._extract_timestamp_from_element(element)

            # 2. ìƒí˜¸ì‘ìš© ë°ì´í„° ì¶”ì¶œ
            likes, comments = await self._extract_interactions_from_element(element)

            # 3. ë°ì´í„° ì¡°í•©
            post_data = {
                "author": subreddit or "Unknown",
                "content": title or "No title",
                "timestamp": timestamp,
                "url": url,
                "likes": likes,
                "comments": comments,
                "shares": None,
            }

            # ìœ íš¨ì„± ê²€ì‚¬
            if not title and not url:
                return None

            if self.debug_mode:
                typer.echo(
                    f"   âœ… article ì¶”ì¶œ: {post_data['author']} - {post_data['content'][:30]}..."
                )

            return post_data

        except Exception as e:
            if self.debug_mode:
                typer.echo(f"   âŒ article ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def _extract_title_from_element(self, element) -> Optional[str]:
        """ìš”ì†Œì—ì„œ ì œëª© ì¶”ì¶œ"""
        # h3 íƒœê·¸ì—ì„œ ì œëª© ì°¾ê¸°
        try:
            h3_elements = await element.locator("h3").all()
            for h3 in h3_elements:
                h3_text = await h3.inner_text()
                if h3_text and len(h3_text) > 5:
                    return h3_text.strip()
        except Exception:
            pass

        # a íƒœê·¸ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì œëª© ì°¾ê¸°
        try:
            links = await element.locator('a[href*="/comments/"]').all()
            for link in links:
                link_text = await link.inner_text()
                if link_text and len(link_text) > 5:
                    return link_text.strip()
        except Exception:
            pass

        return None

    async def _extract_subreddit_from_element(self, element) -> Optional[str]:
        """ìš”ì†Œì—ì„œ ì„œë¸Œë ˆë”§ ì¶”ì¶œ"""
        try:
            subreddit_links = await element.locator('a[href^="/r/"]').all()
            for link in subreddit_links:
                href = await link.get_attribute("href")
                if href and "/comments/" not in href:
                    match = re.search(r"/r/([^/]+)", href)
                    if match:
                        return f"r/{match.group(1)}"
        except Exception:
            pass
        return None

    async def _extract_url_from_element(self, element) -> Optional[str]:
        """ìš”ì†Œì—ì„œ URL ì¶”ì¶œ"""
        try:
            comment_links = await element.locator('a[href*="/comments/"]').all()
            if comment_links:
                href = await comment_links[0].get_attribute("href")
                if href:
                    return f"https://www.reddit.com{href}" if href.startswith("/") else href
        except Exception:
            pass
        return None

    async def _extract_timestamp_from_element(self, element) -> str:
        """ìš”ì†Œì—ì„œ ì‹œê°„ ì¶”ì¶œ"""
        try:
            time_elements = await element.locator("time").all()
            if time_elements:
                return await time_elements[0].inner_text()
        except Exception:
            pass
        return ""

    async def _extract_interactions_from_element(self, element) -> tuple[int, int]:
        """ìš”ì†Œì—ì„œ ìƒí˜¸ì‘ìš© ë°ì´í„° ì¶”ì¶œ"""
        likes = 0
        comments = 0

        try:
            text = await element.inner_text()

            # ì—…ë³´íŠ¸ íŒ¨í„´
            upvote_patterns = [
                r"(\d+\.?\d*[KkMm]?)\s*upvote",
                r"Vote.*?(\d+\.?\d*[KkMm]?)",
                r"^(\d+\.?\d*[KkMm]?)$",  # ìˆ«ìë§Œ ìˆëŠ” ë¼ì¸
            ]

            for pattern in upvote_patterns:
                match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)
                if match:
                    likes = self._parse_number_from_text(match.group(1))
                    break

            # ëŒ“ê¸€ íŒ¨í„´
            comment_patterns = [
                r"(\d+\.?\d*[KkMm]?)\s*comment",
                r"ğŸ’¬\s*(\d+\.?\d*[KkMm]?)",
            ]

            for pattern in comment_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    comments = self._parse_number_from_text(match.group(1))
                    break

        except Exception:
            pass

        return likes, comments

    async def _extract_post_data_from_shreddit(self, element) -> Optional[Dict[str, Any]]:
        """shreddit-post ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # 1. ì†ì„± ë””ë²„ê·¸ (í•„ìš”ì‹œ)
            if self.debug_mode:
                await self._debug_shreddit_attributes(element)

            # 2. ê¸°ë³¸ ì†ì„± ì¶”ì¶œ
            attrs = await self._extract_shreddit_attributes(element)

            # 3. ê° ë°ì´í„° ì¶”ì¶œ
            title = await self._extract_shreddit_title(element, attrs.get("post_title"))
            subreddit = await self._extract_shreddit_subreddit(element, attrs.get("subreddit_name"))
            timestamp = await self._extract_shreddit_timestamp(
                element, attrs.get("created_timestamp")
            )
            upvotes = await self._extract_shreddit_upvotes(element, attrs.get("score"))

            # 4. URL ë° ëŒ“ê¸€ìˆ˜ ì²˜ë¦¬
            url = (
                f"https://www.reddit.com{attrs.get('permalink')}"
                if attrs.get("permalink")
                else None
            )
            comments = self._parse_number_safe(attrs.get("comment_count", 0))

            # 5. ë°ì´í„° ì¡°í•©
            post_data = {
                "author": subreddit or "Unknown",
                "content": title or "No title",
                "timestamp": timestamp or "",
                "url": url,
                "likes": upvotes,
                "comments": comments,
                "shares": None,
            }

            # 6. ì œëª©ì´ ì—†ëŠ” ê²½ìš° fallback
            if not title or title == "No title":
                fallback_title = await self._extract_fallback_title(element)
                if fallback_title:
                    post_data["content"] = fallback_title

            if self.debug_mode:
                typer.echo(
                    f"   âœ… ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {post_data['author']} - {post_data['content'][:30]}..."
                )

            return post_data

        except Exception as e:
            if self.debug_mode:
                typer.echo(f"   âŒ ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def _extract_post_data(self, element) -> Optional[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ìš”ì†Œì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            author = await self._extract_author(element)
            content = await self._extract_content(element)

            # ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
            if self.debug_mode:
                typer.echo(f"   ğŸ” ì¶”ì¶œ ë°ì´í„°: author='{author}', content='{content[:50]}...'")

            # ì €ìë‚˜ ì½˜í…ì¸  ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ìœ íš¨í•œ ê²Œì‹œê¸€ë¡œ íŒë‹¨ (ì¡°ê±´ ì™„í™”)
            if not author and not content:
                if self.debug_mode:
                    typer.echo("   âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ê²Œì‹œê¸€ ê±´ë„ˆëœ€")
                return None

            interactions = await self._extract_interactions(element)

            post_data = {
                "author": author or "Unknown",
                "content": content or "No title",
                "timestamp": await self._extract_timestamp(element),
                "url": await self._extract_url(element),
                "likes": interactions.get("likes", 0),
                "comments": interactions.get("comments", 0),
                "shares": None,  # Redditì€ ê³µìœ  ìˆ˜ë¥¼ ì§ì ‘ í‘œì‹œí•˜ì§€ ì•ŠìŒ
            }

            if self.debug_mode:
                typer.echo(
                    f"   âœ… ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {post_data['author']} - {post_data['content'][:30]}..."
                )

            return post_data
        except Exception as e:
            if self.debug_mode:
                typer.echo(f"   âŒ ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def _extract_author(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ ì‘ì„±ì ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶˜ ê°œì„ """
        try:
            # Reddit êµ¬ì¡°ì—ì„œ ì„œë¸Œë ˆë”§ ì •ë³´ ì¶”ì¶œ (r/subreddit í˜•íƒœ)
            subreddit_selectors = [
                'a[href*="/r/"]',  # r/subreddit ë§í¬
                'link[href*="/r/"]',  # generic link í˜•íƒœ
            ]

            for selector in subreddit_selectors:
                try:
                    subreddit_element = await element.query_selector(selector)
                    if subreddit_element:
                        href = await subreddit_element.get_attribute("href")
                        if href and "/r/" in href:
                            # /r/subreddit í˜•íƒœì—ì„œ ì„œë¸Œë ˆë”§ëª… ì¶”ì¶œ
                            subreddit_name = href.split("/r/")[-1].split("/")[0]
                            if subreddit_name:
                                return f"r/{subreddit_name}"
                except Exception:
                    continue

            # ì„œë¸Œë ˆë”§ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì°¾ê¸°
            text_content = await element.inner_text()
            if "r/" in text_content:
                subreddit_match = re.search(r"r/([a-zA-Z0-9_]+)", text_content)
                if subreddit_match:
                    return f"r/{subreddit_match.group(1)}"

        except Exception:
            pass
        return "Unknown"

    async def _extract_content(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ ì½˜í…ì¸ (ì œëª©) ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶˜ ê°œì„ """
        try:
            # 1. article íƒœê·¸ì˜ aria-label ì†ì„± í™•ì¸ (ê°€ì¥ ì •í™•í•œ ë°©ë²•)
            try:
                article_aria_label = await element.get_attribute("aria-label")
                if article_aria_label and len(article_aria_label) > 3:
                    return article_aria_label.strip()
            except Exception:
                pass

            # 2. Reddit êµ¬ì¡°ì—ì„œ ì œëª©ì€ heading íƒœê·¸ì— ìˆìŒ (level=2)
            title_selectors = [
                'heading[level="2"]',  # ì •í™•í•œ heading íƒœê·¸
                "h2",  # ì¼ë°˜ì ì¸ h2 íƒœê·¸
                "h3",  # ëŒ€ì²´ìš© h3 íƒœê·¸
            ]

            for selector in title_selectors:
                try:
                    title_element = await element.query_selector(selector)
                    if title_element:
                        title_text = (await title_element.inner_text()).strip()
                        if title_text and len(title_text) > 3:
                            return title_text
                except Exception:
                    continue

            # 3. ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
            link_selectors = [
                'a[href*="/comments/"]',  # ëŒ“ê¸€ ë§í¬
                'a[href*="/r/"]',  # ì„œë¸Œë ˆë”§ ë§í¬
            ]

            for selector in link_selectors:
                try:
                    link_element = await element.query_selector(selector)
                    if link_element:
                        link_text = (await link_element.inner_text()).strip()
                        if link_text and not link_text.startswith("r/") and len(link_text) > 3:
                            return link_text
                except Exception:
                    continue

        except Exception:
            pass
        return ""

    async def _extract_timestamp(self, element) -> str:
        """ê²Œì‹œê¸€ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶˜ ê°œì„ """
        try:
            # Reddit êµ¬ì¡°ì—ì„œ ì‹œê°„ ì •ë³´ëŠ” time íƒœê·¸ì— ìˆìŒ
            timestamp_selectors = [
                "time",  # ì‹¤ì œ time íƒœê·¸
                'span[class*="timestamp"]',  # ëŒ€ì²´ìš© ì„ íƒì
                'span[class*="time"]',  # ì‹œê°„ ê´€ë ¨ span
            ]

            for selector in timestamp_selectors:
                try:
                    time_element = await element.query_selector(selector)
                    if time_element:
                        time_text = (await time_element.inner_text()).strip()
                        if time_text:
                            return time_text
                except Exception:
                    continue

            # ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ì°¾ê¸°
            text_content = await element.inner_text()
            if text_content:
                # "X hr. ago", "X min. ago", "X days ago" ë“±ì˜ íŒ¨í„´ ì°¾ê¸°
                time_patterns = [
                    r"(\d+)\s+(hr|hour|hours)\.?\s+ago",
                    r"(\d+)\s+(min|minute|minutes)\.?\s+ago",
                    r"(\d+)\s+(day|days)\.?\s+ago",
                    r"(\d+)\s+(sec|second|seconds)\.?\s+ago",
                ]

                for pattern in time_patterns:
                    match = re.search(pattern, text_content, re.IGNORECASE)
                    if match:
                        return match.group(0)

        except Exception:
            pass
        return ""

    async def _extract_url(self, element) -> Optional[str]:
        """ê²Œì‹œê¸€ì—ì„œ URL ì¶”ì¶œ"""
        url_selectors = [
            'a[data-testid="post_title"]',
            'a[data-click-id="body"]',
            'a[href*="/comments/"]',
        ]
        try:
            for selector in url_selectors:
                try:
                    url_element = await element.query_selector(selector)
                    if url_element:
                        href = await url_element.get_attribute("href")
                        if href and href.startswith("/"):
                            return self.base_url + href
                        return href
                except Exception:
                    continue
        except Exception:
            pass
        return None

    async def _extract_interactions(self, element) -> Dict[str, int]:
        """ê²Œì‹œê¸€ì—ì„œ ìƒí˜¸ì‘ìš©(ì—…ë³´íŠ¸, ëŒ“ê¸€) ë°ì´í„° ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì— ë§ì¶˜ ê°œì„ """
        interactions = {"likes": 0, "comments": 0}

        try:
            # ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            text_content = await element.inner_text()
            if text_content:
                # 1. ì—…ë³´íŠ¸ ìˆ˜ ì¶”ì¶œ - ì‹¤ì œ Reddit êµ¬ì¡°ì—ì„œ íŒ¨í„´ ì°¾ê¸°
                upvote_patterns = [
                    r"Upvote\s+(\d+\.?\d*[KM]?)\s+Downvote",  # "Upvote 307 Downvote"
                    r"generic:\s*\"(\d+\.?\d*[KM]?)\"\s+.*Downvote",  # "generic: "307" ... Downvote"
                    r"(\d+\.?\d*[KM]?)\s+Go to comments",  # ë•Œë¡œëŠ” ì—…ë³´íŠ¸ ìˆ˜ê°€ ëŒ“ê¸€ ì „ì— ë‚˜íƒ€ë‚¨
                ]

                for pattern in upvote_patterns:
                    match = re.search(pattern, text_content)
                    if match:
                        interactions["likes"] = self._parse_number_from_text(match.group(1))
                        break

                # 2. ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ - "Go to comments" ë§í¬ì—ì„œ ìˆ«ì ì°¾ê¸°
                comment_patterns = [
                    r"(\d+\.?\d*[KM]?)\s+Go to comments",  # "67 Go to comments"
                    r"link\s+\"(\d+\.?\d*[KM]?)\s+Go to comments\"",  # ë§í¬ ë‚´ í…ìŠ¤íŠ¸
                ]

                for pattern in comment_patterns:
                    match = re.search(pattern, text_content)
                    if match:
                        interactions["comments"] = self._parse_number_from_text(match.group(1))
                        break

                # 3. ëŒ€ì²´ ë°©ë²• - ê° ë¼ì¸ì—ì„œ ìˆ«ì ì°¾ê¸°
                if interactions["likes"] == 0 or interactions["comments"] == 0:
                    lines = text_content.split("\n")
                    for i, line in enumerate(lines):
                        line = line.strip()

                        # ì—…ë³´íŠ¸ ìˆ˜ ì°¾ê¸°
                        if interactions["likes"] == 0:
                            if "Upvote" in line and "Downvote" in line:
                                numbers = re.findall(r"(\d+\.?\d*[KM]?)", line)
                                if numbers:
                                    interactions["likes"] = self._parse_number_from_text(numbers[0])
                            elif line.isdigit() or (re.match(r"^\d+\.?\d*[KM]?$", line)):
                                # ë‹¤ìŒ ë¼ì¸ì´ Downvoteì¸ì§€ í™•ì¸
                                next_line = lines[i + 1] if i + 1 < len(lines) else ""
                                if "Downvote" in next_line:
                                    interactions["likes"] = self._parse_number_from_text(line)

                        # ëŒ“ê¸€ ìˆ˜ ì°¾ê¸°
                        if interactions["comments"] == 0 and "Go to comments" in line:
                            numbers = re.findall(r"(\d+\.?\d*[KM]?)", line)
                            if numbers:
                                interactions["comments"] = self._parse_number_from_text(numbers[0])

        except Exception:
            pass

        return interactions

    def _parse_number_from_text(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì íŒŒì‹± (ì˜ˆ: '1.7k' -> 1700)"""
        if not text:
            return 0
        text = text.lower().strip()
        try:
            if "k" in text:
                num_part = text.replace("k", "").strip()
                return int(float(num_part) * 1000)
            if "m" in text:
                num_part = text.replace("m", "").strip()
                return int(float(num_part) * 1_000_000)

            # ìˆ«ìë§Œ ìˆëŠ”ì§€ í™•ì¸
            cleaned_text = re.sub(r"[^\d.]", "", text)
            if cleaned_text:
                return int(float(cleaned_text))

        except (ValueError, TypeError):
            return 0
        return 0

    async def _scroll_for_more_posts(self, page: Page):
        """ë” ë§ì€ ê²Œì‹œê¸€ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ìŠ¤í¬ë¡¤"""
        try:
            typer.echo("   ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤...")
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            # ìŠ¤í¬ë¡¤ í›„ ì½˜í…ì¸ ê°€ ë¡œë“œë  ì‹œê°„ì„ ì¤Œ
            await page.wait_for_timeout(3000)
        except Exception as e:
            typer.echo(f"   âš ï¸ ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    async def _load_session(self, page: Page) -> bool:
        """ì €ì¥ëœ ì„¸ì…˜ ë¡œë“œ"""
        if self.session_path.exists():
            typer.echo("ğŸ’¾ ì €ì¥ëœ ì„¸ì…˜ ë¡œë“œ ì¤‘...")
            try:
                # ì„¸ì…˜ íŒŒì¼ ë¡œë“œ
                with open(self.session_path, "r", encoding="utf-8") as f:
                    session_data = json.load(f)

                # ì„¸ì…˜ ì ìš©
                await page.context.add_cookies(session_data.get("cookies", []))

                # localStorage ë° sessionStorage ì ìš©
                if "origins" in session_data:
                    for origin in session_data["origins"]:
                        if origin.get("origin") == "https://www.reddit.com":
                            # localStorage ì„¤ì •
                            if "localStorage" in origin:
                                await page.goto(
                                    "https://www.reddit.com", wait_until="domcontentloaded"
                                )
                                for item in origin["localStorage"]:
                                    await page.evaluate(
                                        f'window.localStorage.setItem("{item["name"]}", {json.dumps(item["value"])})'
                                    )

                # ì„¸ì…˜ ìœ íš¨ì„± ê²€ì‚¬
                await page.goto(self.base_url, wait_until="domcontentloaded")
                await page.wait_for_timeout(2000)

                # ë¡œê·¸ì¸ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìš”ì†Œ í™•ì¸ - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
                login_indicators = [
                    'button[aria-label*="Expand user menu"]',
                    'button[id*="USER_DROPDOWN"]',
                    'div[class*="header-user-dropdown"]',
                    'button[aria-label*="profile"]',
                    'a[href="/submit"]',  # Create Post ë²„íŠ¼
                    'button:has-text("Create Post")',
                ]

                for indicator in login_indicators:
                    try:
                        element = page.locator(indicator).first
                        if await element.is_visible():
                            typer.echo(f"   âœ… ì„¸ì…˜ ìœ íš¨í•¨, ë¡œê·¸ì¸ ê±´ë„ˆëœ€ (í™•ì¸: {indicator})")
                            return True
                    except Exception:
                        continue

                typer.echo("   âš ï¸ ì„¸ì…˜ ë§Œë£Œë¨, ì¬ë¡œê·¸ì¸ í•„ìš”")
                return False

            except Exception as e:
                typer.echo(f"   âŒ ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # ì†ìƒëœ ì„¸ì…˜ íŒŒì¼ ì‚­ì œ
                try:
                    self.session_path.unlink()
                    typer.echo("   ğŸ—‘ï¸ ì†ìƒëœ ì„¸ì…˜ íŒŒì¼ ì‚­ì œë¨")
                except Exception:
                    pass
                return False
        return False

    async def _save_session(self, page: Page):
        """í˜„ì¬ ì„¸ì…˜ ì €ì¥"""
        try:
            typer.echo("ğŸ’¾ í˜„ì¬ ì„¸ì…˜ ì €ì¥ ì¤‘...")
            # ì„¸ì…˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            self.session_path.parent.mkdir(parents=True, exist_ok=True)
            await page.context.storage_state(path=self.session_path)
            typer.echo("   âœ… ì„¸ì…˜ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            typer.echo(f"   âŒ ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _save_debug_html(self, page: Page, filename: str):
        """ë””ë²„ê·¸ìš© HTML íŒŒì¼ ì €ì¥"""
        if self.debug_mode:
            debug_path = Path("data/debug/reddit")
            debug_path.mkdir(parents=True, exist_ok=True)
            full_path = debug_path / filename
            try:
                content = await page.content()
                with open(full_path, "w", encoding="utf-8") as f:
                    f.write(content)
                typer.echo(f"   ğŸ› ë””ë²„ê·¸ HTML ì €ì¥: {full_path}")
            except Exception as e:
                typer.echo(f"   âŒ ë””ë²„ê·¸ HTML ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _debug_shreddit_attributes(self, element):
        """ë””ë²„ê·¸: shreddit-post ì†ì„± í™•ì¸"""
        typer.echo("   ğŸ” shreddit-post ì†ì„± í™•ì¸ ì¤‘...")
        try:
            attrs = await element.evaluate(
                """
                (el) => {
                    const attrs = {};
                    for (const attr of el.attributes) {
                        attrs[attr.name] = attr.value;
                    }
                    return attrs;
                }
            """
            )
            if attrs:
                typer.echo(f"      ì†ì„±ë“¤: {list(attrs.keys())[:10]}...")
        except Exception as e:
            typer.echo(f"      ì†ì„± í™•ì¸ ì‹¤íŒ¨: {e}")

    async def _extract_shreddit_attributes(self, element) -> Dict[str, Any]:
        """ì£¼ìš” shreddit-post ì†ì„± ì¶”ì¶œ"""
        attrs = {}
        attrs["permalink"] = await element.get_attribute("permalink")
        attrs["comment_count"] = await element.get_attribute("comment-count")
        attrs["created_timestamp"] = await element.get_attribute("created-timestamp")
        attrs["post_title"] = await element.get_attribute("post-title")
        attrs["subreddit_name"] = await element.get_attribute("subreddit-name")
        attrs["score"] = await element.get_attribute("score")

        if self.debug_mode:
            typer.echo(f"      permalink: {attrs['permalink']}")
            typer.echo(f"      comment-count: {attrs['comment_count']}")

        return attrs

    async def _extract_shreddit_title(self, element, attr_title: Optional[str]) -> Optional[str]:
        """ì œëª© ì¶”ì¶œ"""
        # 1. ì†ì„±ì—ì„œ
        if attr_title:
            if self.debug_mode:
                typer.echo(f"      ì œëª©(ì†ì„±): {attr_title[:50]}...")
            return attr_title

        # 2. ëŒ“ê¸€ ë§í¬ì—ì„œ
        try:
            title_links = await element.locator('a[href*="/comments/"]').all()
            for link in title_links:
                link_text = await link.inner_text()
                if link_text and len(link_text) > 5:
                    if self.debug_mode:
                        typer.echo(f"      ì œëª©(ë§í¬): {link_text[:50]}...")
                    return link_text
        except Exception:
            pass

        # 3. í—¤ë”© íƒœê·¸ì—ì„œ
        for heading in ["h1", "h2", "h3"]:
            try:
                heading_el = element.locator(heading).first
                if await heading_el.count() > 0:
                    title = await heading_el.inner_text()
                    if title:
                        if self.debug_mode:
                            typer.echo(f"      ì œëª©({heading}): {title[:50]}...")
                        return title
            except Exception:
                continue

        return None

    async def _extract_shreddit_subreddit(
        self, element, attr_subreddit: Optional[str]
    ) -> Optional[str]:
        """ì„œë¸Œë ˆë”§ ì¶”ì¶œ"""
        # 1. ì†ì„±ì—ì„œ
        if attr_subreddit:
            return f"r/{attr_subreddit}"

        # 2. ë§í¬ì—ì„œ
        try:
            subreddit_links = await element.locator(
                'a[href^="/r/"]:not([href*="/comments/"])'
            ).all()
            for link in subreddit_links:
                href = await link.get_attribute("href")
                if href:
                    match = re.search(r"/r/([^/]+)", href)
                    if match:
                        return f"r/{match.group(1)}"
        except Exception:
            pass

        return None

    async def _extract_shreddit_timestamp(self, element, attr_timestamp: Optional[str]) -> str:
        """ì‹œê°„ ì¶”ì¶œ"""
        # time íƒœê·¸ì—ì„œ
        try:
            time_elements = await element.locator("time").all()
            if time_elements:
                return await time_elements[0].inner_text()
        except Exception:
            pass

        # ì†ì„±ì—ì„œ
        return attr_timestamp or ""

    async def _extract_shreddit_upvotes(self, element, attr_score: Optional[str]) -> int:
        """ì—…ë³´íŠ¸ ìˆ˜ ì¶”ì¶œ"""
        # 1. ì†ì„±ì—ì„œ
        if attr_score:
            upvotes = self._parse_number_safe(attr_score)
            if upvotes > 0:
                return upvotes

        # 2. faceplate-numberì—ì„œ
        try:
            faceplate_numbers = await element.locator("faceplate-number").all()
            for fn in faceplate_numbers:
                number_attr = await fn.get_attribute("number")
                if number_attr:
                    upvotes = self._parse_number_safe(number_attr)
                    if upvotes > 0:
                        return upvotes
        except Exception:
            pass

        # 3. í…ìŠ¤íŠ¸ì—ì„œ
        try:
            text_content = await element.inner_text()
            upvote_match = re.search(r"(\d+\.?\d*[KkMm]?)\s*upvote", text_content, re.IGNORECASE)
            if upvote_match:
                return self._parse_number_from_text(upvote_match.group(1))
        except Exception:
            pass

        return 0

    async def _extract_fallback_title(self, element) -> Optional[str]:
        """ì œëª©ì´ ì—†ëŠ” ê²½ìš° fallback ì¶”ì¶œ"""
        try:
            full_text = await element.inner_text()
            lines = [line.strip() for line in full_text.split("\n") if line.strip()]

            for line in lines:
                if (
                    not line.startswith("r/")
                    and not re.match(r"^\d+\.?\d*[KkMm]?\s*(upvote|comment)", line, re.IGNORECASE)
                    and len(line) > 10
                ):
                    if self.debug_mode:
                        typer.echo(f"      ì œëª©(í…ìŠ¤íŠ¸ ì¶”ì¶œ): {line[:50]}...")
                    return line
        except Exception:
            pass
        return None

    def _parse_number_safe(self, value: str) -> int:
        """ì•ˆì „í•œ ìˆ«ì íŒŒì‹±"""
        try:
            return int(value)
        except (ValueError, TypeError):
            return 0
