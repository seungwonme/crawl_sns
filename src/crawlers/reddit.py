"""
@file reddit.py
@description Reddit í”Œë«í¼ ì „ìš© í¬ë¡¤ëŸ¬

ì´ ëª¨ë“ˆì€ Reddit í”Œë«í¼ì—ì„œ ê²Œì‹œê¸€ì„ í¬ë¡¤ë§í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. Reddit í”¼ë“œì—ì„œ ê²Œì‹œê¸€ ìˆ˜ì§‘
2. Reddit ê³„ì •ì„ í†µí•œ ë¡œê·¸ì¸ ì§€ì›
3. ì‘ì„±ì, ì½˜í…ì¸ , ì—…ë³´íŠ¸/ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ
4. ì„¸ì…˜ ê´€ë¦¬ (ì¬ë¡œê·¸ì¸ ë°©ì§€)
5. ì ì§„ì  ì¶”ì¶œ ì‹œìŠ¤í…œ (ìŠ¤í¬ë¡¤ë§, ì„œë¸Œë ˆë”§ íƒìƒ‰)

í•µì‹¬ êµ¬í˜„ ë¡œì§:
- Reddit ë¡œê·¸ì¸ì„ í†µí•œ í”¼ë“œ ì ‘ê·¼
- ì ì§„ì  ìŠ¤í¬ë¡¤ë§ìœ¼ë¡œ ë” ë§ì€ ê²Œì‹œê¸€ ë¡œë“œ
- div[data-testid="post-container"] ê¸°ë°˜ ê²Œì‹œê¸€ ì¶”ì¶œ
- K/M ë‹¨ìœ„ ì—…ë³´íŠ¸ ìˆ˜ì¹˜ íŒŒì‹±

@dependencies
- playwright.async_api: ë¸Œë¼ìš°ì € ìë™í™”
- typer: CLI ì¶œë ¥
- .base: ë² ì´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤

@see {@link https://www.reddit.com} - Reddit í”Œë«í¼
"""

import os
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import typer
from dotenv import load_dotenv
from playwright.async_api import Page
from playwright.async_api import TimeoutError as PlaywrightTimeoutError

from ..models import Post
from .base import BaseCrawler

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class RedditCrawler(BaseCrawler):
    """Reddit ì „ìš© í¬ë¡¤ëŸ¬"""

    def __init__(self, debug_mode: bool = False):
        super().__init__(
            platform_name="reddit", base_url="https://www.reddit.com", debug_mode=debug_mode
        )
        self.username = os.getenv("REDDIT_USERNAME")
        self.password = os.getenv("REDDIT_PASSWORD")
        self.session_path = Path("data/reddit_session.json")
        self.max_scroll_attempts = 10

        if not self.username or not self.password:
            raise ValueError("REDDIT_USERNAMEê³¼ REDDIT_PASSWORD í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    async def _crawl_implementation(self, page: Page, count: int) -> List[Post]:
        """Reddit ê²Œì‹œê¸€ í¬ë¡¤ë§ êµ¬í˜„"""
        try:
            # ì„¸ì…˜ ë¡œë“œ ì‹œë„
            if not await self._load_session(page):
                # ì„¸ì…˜ì´ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ë¡œê·¸ì¸
                await self._login(page)
                await self._save_session(page)

            # ë©”ì¸ í”¼ë“œë¡œ ì´ë™
            await page.goto("https://www.reddit.com/", wait_until="domcontentloaded")
            await page.wait_for_timeout(3000)

            # ê²Œì‹œê¸€ ìˆ˜ì§‘
            posts = await self._progressive_post_collection(page, count)

            if self.debug_mode:
                await self._save_debug_html(page, "reddit_posts.html")

            return posts

        except Exception as e:
            typer.echo(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.debug_mode:
                await self._save_debug_html(page, "reddit_error.html")
            return []

    async def crawl(self, count: int = 10) -> List[Post]:
        """Reddit ê²Œì‹œê¸€ í¬ë¡¤ë§ - ë² ì´ìŠ¤ í´ë˜ìŠ¤ ì˜¤ë²„ë¼ì´ë“œ"""
        typer.echo(f"ğŸ”´ Reddit í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {count}ê°œ)")
        return await super().crawl(count)

    async def _login(self, page: Page) -> bool:
        """Reddit ë¡œê·¸ì¸"""
        try:
            typer.echo("ğŸ”‘ Reddit ë¡œê·¸ì¸ ì¤‘...")
            await page.goto("https://www.reddit.com/login/", wait_until="domcontentloaded")
            await page.wait_for_timeout(2000)

            # ì‚¬ìš©ìëª… ì…ë ¥
            username_input = await page.wait_for_selector("#loginUsername", timeout=10000)
            if username_input and self.username:
                await username_input.fill(self.username)

            # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
            password_input = await page.wait_for_selector("#loginPassword", timeout=5000)
            if password_input and self.password:
                await password_input.fill(self.password)

            # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
            login_button = await page.wait_for_selector('button[type="submit"]', timeout=5000)
            if login_button:
                await login_button.click()

            # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸°
            try:
                await page.wait_for_url("https://www.reddit.com/", timeout=15000)
                typer.echo("âœ… Reddit ë¡œê·¸ì¸ ì„±ê³µ!")
                return True
            except PlaywrightTimeoutError:
                # ëŒ€ì•ˆ: ë¡œê·¸ì¸ í›„ í™ˆí˜ì´ì§€ë¡œ ì´ë™í–ˆëŠ”ì§€ í™•ì¸
                current_url = page.url
                if "reddit.com" in current_url and "login" not in current_url:
                    typer.echo("âœ… Reddit ë¡œê·¸ì¸ ì„±ê³µ!")
                    return True
                else:
                    typer.echo("âŒ Reddit ë¡œê·¸ì¸ ì‹¤íŒ¨")
                    return False

        except Exception as e:
            typer.echo(f"âŒ ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    async def _progressive_post_collection(self, page: Page, target_count: int) -> List[Post]:
        """ì ì§„ì  ê²Œì‹œê¸€ ìˆ˜ì§‘"""
        posts = []
        scroll_attempts = 0

        typer.echo(f"ğŸ”„ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)")

        while len(posts) < target_count and scroll_attempts < self.max_scroll_attempts:
            # í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ì¶”ì¶œ
            current_posts = await self._collect_posts(page)

            # ìƒˆë¡œìš´ ê²Œì‹œê¸€ë§Œ ì¶”ê°€
            new_posts_count = 0
            for post_data in current_posts:
                if len(posts) >= target_count:
                    break

                # ì¤‘ë³µ í™•ì¸
                is_duplicate = any(
                    existing_post.url == post_data.get("url")
                    or (
                        existing_post.content == post_data.get("content")
                        and existing_post.author == post_data.get("author")
                    )
                    for existing_post in posts
                )

                if not is_duplicate:
                    post = Post(
                        platform="reddit",
                        author=post_data.get("author", "Unknown"),
                        content=post_data.get("content", ""),
                        timestamp=post_data.get("timestamp", ""),
                        url=post_data.get("url"),
                        likes=post_data.get("likes"),
                        comments=post_data.get("comments"),
                        shares=post_data.get("shares"),
                    )
                    posts.append(post)
                    new_posts_count += 1

            typer.echo(f"   ğŸ“Š ìˆ˜ì§‘ í˜„í™©: {len(posts)}/{target_count} (+{new_posts_count}ê°œ ì‹ ê·œ)")

            if len(posts) >= target_count:
                break

            if new_posts_count == 0:
                typer.echo("   âš ï¸ ìƒˆë¡œìš´ ê²Œì‹œê¸€ì´ ì—†ìŒ")

            # í˜ì´ì§€ ìŠ¤í¬ë¡¤
            await self._scroll_for_more_posts(page)
            scroll_attempts += 1

        typer.echo(f"âœ… ì´ {len(posts)}ê°œ ê²Œì‹œê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        return posts

    async def _collect_posts(self, page: Page) -> List[Dict[str, Any]]:
        """í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œê¸€ ìˆ˜ì§‘"""
        posts = []

        # Reddit ê²Œì‹œê¸€ ì„ íƒìë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        post_selectors = [
            'div[data-testid="post-container"]',
            "article",
            'div[data-click-id="body"]',
            ".Post",
        ]

        post_elements = None
        for selector in post_selectors:
            post_elements = await page.query_selector_all(selector)
            if post_elements:
                break

        if not post_elements:
            return posts

        for element in post_elements:
            try:
                post_data = await self._extract_post_data(element)
                if post_data:
                    posts.append(post_data)
            except Exception as e:
                if self.debug_mode:
                    typer.echo(f"   âš ï¸ ê²Œì‹œê¸€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                continue

        return posts

    async def _extract_post_data(self, element) -> Optional[Dict[str, Any]]:
        """ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            author = await self._extract_author(element)
            content = await self._extract_content(element)
            timestamp = await self._extract_timestamp(element)
            url = await self._extract_url(element)
            interactions = await self._extract_interactions(element)

            # ìµœì†Œ ìš”êµ¬ì‚¬í•­ í™•ì¸
            if not author or not content:
                return None

            post_data = {
                "author": author,
                "content": content,
                "timestamp": timestamp,
                "url": url,
                **interactions,
            }

            return post_data

        except Exception as e:
            if self.debug_mode:
                typer.echo(f"   âš ï¸ ê²Œì‹œê¸€ ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    async def _extract_author(self, element) -> str:
        """ì‘ì„±ì ì¶”ì¶œ"""
        author_selectors = [
            '[data-testid="post_author_link"]',
            'a[href*="/user/"]',
            ".author",
            '[data-click-id="user"]',
        ]

        for selector in author_selectors:
            try:
                author_element = await element.query_selector(selector)
                if author_element:
                    author_text = await author_element.inner_text()
                    if author_text:
                        # Reddit ì‚¬ìš©ìëª… ì •ì œ (u/username -> username)
                        author_text = author_text.strip()
                        if author_text.startswith("u/"):
                            author_text = author_text[2:]
                        return author_text
            except:
                continue

        return "Unknown"

    async def _extract_content(self, element) -> str:
        """ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ"""
        content_selectors = [
            '[data-testid="post-content"]',
            ".RichTextJSON-root",
            '[data-click-id="text"]',
            'div[data-adclicklocation="title"]',
            ".title",
            "h3",
        ]

        for selector in content_selectors:
            try:
                content_element = await element.query_selector(selector)
                if content_element:
                    content_text = await content_element.inner_text()
                    if content_text:
                        return content_text.strip()
            except:
                continue

        # ëŒ€ì•ˆ: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
        try:
            full_text = await element.inner_text()
            lines = [line.strip() for line in full_text.split("\n") if line.strip()]

            # ì²« ë²ˆì§¸ ì˜ë¯¸ìˆëŠ” ë¼ì¸ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
            for line in lines:
                if len(line) > 10 and not line.startswith(("u/", "r/", "â€¢")):
                    return line[:500]  # 500ìë¡œ ì œí•œ
        except:
            pass

        return ""

    async def _extract_timestamp(self, element) -> str:
        """ê²Œì‹œ ì‹œê°„ ì¶”ì¶œ"""
        timestamp_selectors = [
            "time",
            '[data-testid="post_timestamp"]',
            'a[data-click-id="timestamp"]',
        ]

        for selector in timestamp_selectors:
            try:
                time_element = await element.query_selector(selector)
                if time_element:
                    # datetime ì†ì„± ìš°ì„ 
                    datetime_attr = await time_element.get_attribute("datetime")
                    if datetime_attr:
                        return datetime_attr

                    # í…ìŠ¤íŠ¸ ë‚´ìš©
                    time_text = await time_element.inner_text()
                    if time_text:
                        return time_text.strip()
            except:
                continue

        return ""

    async def _extract_url(self, element) -> Optional[str]:
        """ê²Œì‹œê¸€ URL ì¶”ì¶œ"""
        url_selectors = [
            'a[data-click-id="comments"]',
            'a[href*="/comments/"]',
            '[data-testid="post-content"] a',
        ]

        for selector in url_selectors:
            try:
                url_element = await element.query_selector(selector)
                if url_element:
                    href = await url_element.get_attribute("href")
                    if href:
                        if href.startswith("/"):
                            return f"https://www.reddit.com{href}"
                        elif href.startswith("https://"):
                            return href
            except:
                continue

        return None

    async def _extract_interactions(self, element) -> Dict[str, Optional[int]]:
        """ìƒí˜¸ì‘ìš© ì •ë³´ ì¶”ì¶œ"""
        interactions: Dict[str, Optional[int]] = {
            "likes": None,
            "comments": None,
            "shares": None,
        }

        try:
            # ì—…ë³´íŠ¸ ìˆ˜ ì¶”ì¶œ
            upvote_selectors = [
                '[data-testid="vote-arrows"] button[aria-label*="upvote"]',
                'button[aria-label*="upvote"]',
                ".upvotes",
                '[data-click-id="upvote"]',
            ]

            for selector in upvote_selectors:
                try:
                    upvote_element = await element.query_selector(selector)
                    if upvote_element:
                        aria_label = await upvote_element.get_attribute("aria-label")
                        if aria_label:
                            upvotes = self._parse_number_from_text(aria_label)
                            if upvotes is not None:
                                interactions["likes"] = upvotes
                                break

                        # ëŒ€ì•ˆ: í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                        upvote_text = await upvote_element.inner_text()
                        if upvote_text:
                            upvotes = self._parse_number_from_text(upvote_text)
                            if upvotes is not None:
                                interactions["likes"] = upvotes
                                break
                except:
                    continue

            # ëŒ“ê¸€ ìˆ˜ ì¶”ì¶œ
            comment_selectors = [
                'a[data-click-id="comments"]',
                'button[aria-label*="comment"]',
                '[data-testid="comment-count"]',
            ]

            for selector in comment_selectors:
                try:
                    comment_element = await element.query_selector(selector)
                    if comment_element:
                        # aria-labelì—ì„œ ì¶”ì¶œ
                        aria_label = await comment_element.get_attribute("aria-label")
                        if aria_label:
                            comments = self._parse_number_from_text(aria_label)
                            if comments is not None:
                                interactions["comments"] = comments
                                break

                        # í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                        comment_text = await comment_element.inner_text()
                        if comment_text:
                            comments = self._parse_number_from_text(comment_text)
                            if comments is not None:
                                interactions["comments"] = comments
                                break
                except:
                    continue

        except Exception as e:
            if self.debug_mode:
                typer.echo(f"   âš ï¸ ìƒí˜¸ì‘ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return interactions

    def _parse_number_from_text(self, text: str) -> Optional[int]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ (K, M ë‹¨ìœ„ ì§€ì›)"""
        if not text:
            return None

        # ìˆ«ì + K/M íŒ¨í„´ ì°¾ê¸°
        patterns = [
            r"(\d+\.?\d*)\s*[kK]",  # 1.2k, 15k
            r"(\d+\.?\d*)\s*[mM]",  # 1.5m, 2m
            r"(\d+)",  # ìˆœìˆ˜ ìˆ«ì
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    number = float(match.group(1))
                    if "k" in text.lower():
                        return int(number * 1000)
                    elif "m" in text.lower():
                        return int(number * 1000000)
                    else:
                        return int(number)
                except:
                    continue

        return None

    async def _scroll_for_more_posts(self, page: Page):
        """ë” ë§ì€ ê²Œì‹œê¸€ì„ ìœ„í•œ ìŠ¤í¬ë¡¤"""
        try:
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(1500)

            # ì¶”ê°€ ìŠ¤í¬ë¡¤
            await page.evaluate("window.scrollBy(0, 500)")
            await page.wait_for_timeout(1000)

            # ë„¤íŠ¸ì›Œí¬ ì™„ë£Œ ëŒ€ê¸°
            try:
                await page.wait_for_load_state("networkidle", timeout=5000)
            except PlaywrightTimeoutError:
                pass

        except Exception as e:
            typer.echo(f"   âš ï¸ ìŠ¤í¬ë¡¤ ì¤‘ ì˜¤ë¥˜: {e}")

    async def _load_session(self, page: Page) -> bool:
        """ì €ì¥ëœ ì„¸ì…˜ ë¡œë“œ"""
        try:
            if self.session_path.exists():
                await page.context.storage_state(path=str(self.session_path))

                # ì„¸ì…˜ ìœ íš¨ì„± í™•ì¸
                await page.goto("https://www.reddit.com/", wait_until="domcontentloaded")
                await page.wait_for_timeout(3000)

                # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
                try:
                    user_menu = await page.query_selector('[data-testid="user-menu"]')
                    if user_menu:
                        typer.echo("âœ… ì €ì¥ëœ Reddit ì„¸ì…˜ ë¡œë“œ ì„±ê³µ")
                        return True
                except:
                    pass

            return False

        except Exception as e:
            typer.echo(f"âš ï¸ ì„¸ì…˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    async def _save_session(self, page: Page):
        """í˜„ì¬ ì„¸ì…˜ ì €ì¥"""
        try:
            # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
            self.session_path.parent.mkdir(exist_ok=True)

            # ì„¸ì…˜ ìƒíƒœ ì €ì¥
            await page.context.storage_state(path=str(self.session_path))
            typer.echo("ğŸ’¾ Reddit ì„¸ì…˜ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            typer.echo(f"âš ï¸ ì„¸ì…˜ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _save_debug_html(self, page: Page, filename: str):
        """ë””ë²„ê·¸ìš© HTML ì €ì¥"""
        try:
            debug_dir = Path("data/debug_screenshots")
            debug_dir.mkdir(exist_ok=True)

            html_content = await page.content()
            html_path = debug_dir / filename

            with open(html_path, "w", encoding="utf-8") as f:
                f.write(html_content)

            typer.echo(f"ğŸ› ë””ë²„ê·¸ HTML ì €ì¥: {html_path}")

        except Exception as e:
            typer.echo(f"âš ï¸ ë””ë²„ê·¸ HTML ì €ì¥ ì‹¤íŒ¨: {e}")
